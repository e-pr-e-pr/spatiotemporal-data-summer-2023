---
title: "Soil Water Content and Ecosystem Respiration Analysis"
author: |
  **Eleanor Percy-Rouhaud**  
  Matriculation Number: 3749872
format:
  html:
    code-fold: true
    css: custom.css
jupyter: python3
execute:
  echo: true
  eval: true
  warning: false
---
```{python}
echo: False

from data_processing_functions import process_data_files, log_transfo_data_dict, metrics_to_csv, filter_level_data_common_time, calculate_regression_metrics, calculate_and_store_all_levels, remove_missing_time_steps, four_levels_filter

import os

# define  data paths
reco_data = './data/reco/'
soil_water_data = './data/swc/'
filtered_station_locations = './data/filtered-fluxnet-station-coords.csv'

# imported functions ready to use
reco_data_dict = process_data_files(reco_data)
log_reco_data_dict = log_transfo_data_dict(reco_data_dict)
swc_data_dict = process_data_files(soil_water_data)

# print("Data processing completed successfully")
```


## Introduction 

This is an analysis of Soil Water Content (SWC) and and Ecosystem Respiration (RECO) data. The variables are important for understanding ecosystem dynamics. SWC is a key component of the hydrological cycle, influencing water availability for plants and soil processes. It directly affects plant growth and productivity, . It also impacts soil structure, microbial activity, and organic matter decomposition. 

RECO is the total respiration from plants, animals, and microorganisms. It is a balance of carbon uptake (photosynthesis) and release (respiration). It can also be viewed as an indicator of ecosystem health.

What makes this dataset in particular interesting is the levels in the SWC dataset. The SWC is measured at 4 different depts in the soil (suurface, 0.3m, 0.5m, and 1m below). While I didnt get too much time to look at this in this project, a goal is to look at the behaviour of SWC at 4 levels, and look for example for cross correlation in that data.  In this project, I did look at the regression metrics between RECO and SWC at different levels and different stations, to get a sense of their relationship. I also looked at the seasonal variation of the different stations and different levels. 

This analysis aims to explore the relationship between SWC and RECO across various stations and levels. By examining these variables together, we can gain insights into how water availability influences carbon fluxes and overall ecosystem functioning.

## Setting up the functions to run the analysis

First, lets import the modules we will need to complete the analysis:

```{python}
# data access and manipulation modules
import pandas as pd 
import numpy as np
import csv
import math

# plotting libraries
import matplotlib.pyplot as plt
import plotly.graph_objects as go # interactive plots
from plotly.subplots import make_subplots
import matplotlib.dates as mdates
import datetime
from datetime import datetime
from matplotlib.backends.backend_pdf import PdfPages # saves multiple individual plots in 1 doc in case of not faceted plot
from collections import defaultdict # for plotting layers of each station
```

Now, lets access the data we will be analysing.

```{python}
reco_data = './data/reco/'
soil_water_data = './data/swc/'
filtered_station_locations = './data/filtered-fluxnet-station-coords.csv'
```

I wrote this code to also work with the many other variables and indices available from FLUXNET, so I wrote a lot of functions that can be used for many of them (rather than having to write a fresh code chunk each time). This is what now follows. Explanations of the function follows before each code block. 

The function below takes the datasets to be analysed, here SWC and RECO, and puts them into a more flexible form for te purposes of analysis and visualisation - a dictionary. The data directly from fluxnet is in the form of individual .txt files for each station. Here, I take these individual files and all their contents, and stitch them together into a dictionary, to be accessed by the station name as a key. 

```{python}

#| eval: false
#| echo: true

def process_data_files(directory):
    data_dict = {}
    for file in os.listdir(directory):
        file_path = os.path.join(directory, file)
        station_name = os.path.splitext(os.path.basename(file_path))[0].replace('_daily_avg', '')

        time = []
        data = []

        with open(file_path, 'r') as datafile:
            plotting = csv.reader(datafile, delimiter='\t')
            next(plotting)  # skips header row
            for ROWS in plotting:
                time.append(datetime.strptime(ROWS[0], '%Y-%m-%d'))
                data.append(float(ROWS[1]))

        data_dict[station_name] = {'time': time, 'data': data}

    return data_dict
  ```

For some variables, it is interesting to take the log of its value; this is what this function does, and it creates a new directionary with the same name + log i.e. log_reco_data_dict.
 ```{python}

#| eval: false
#| echo: true

  def log_transfo_data_dict(original_data_dict):
    log_transformed_dict = {}

    for station_name, data_info in original_data_dict.items():
        log_transformed_data = np.log(data_info['data'])
        log_transformed_dict[station_name] = {'time': data_info['time'], 'data': log_transformed_data}

    return log_transformed_dict
  ```

  The function below takes whichever variables you want - again, here only SWC and RECO - and calculates various regression metrics of them, namely the r sqaured metric, the Pearson correlation coefficient, as well as the linear regression metrics (slope and intercept). 

 ```{python}

#| eval: false
#| echo: true

def calculate_regression_metrics(x, y):
    x = np.array(x)
    y = np.array(y)

    # find indices where either x or y is NaN
    nan_indices = np.isnan(x) | np.isnan(y)

    # exclude NaN values from both x and y
    x_valid = x[~nan_indices]
    y_valid = y[~nan_indices]

    if len(x_valid) == 0 or len(y_valid) == 0:
        # if no valid data points are available, return NaN for all metrics
        return np.nan, np.nan, np.nan, np.nan

    correlation_coefficient = np.corrcoef(x_valid, y_valid)[0, 1]

    # linear regression for slope and intercept
    slope, intercept = np.polyfit(x_valid, y_valid, 1)

    residuals = y_valid - (slope * x_valid + intercept)
    ss_residual = np.sum(residuals**2)
    ss_total = np.sum((y_valid - np.mean(y_valid))**2)
    r_squared = 1 - (ss_residual / ss_total)

    return correlation_coefficient, r_squared, slope, intercept

```

Once the metrics are calculated,  it is necessary to ave them stored somewhere - so here is a function to store them in a dictionary. It extracts the prefix of the station for the first data dictioanry, which indicates its name, and compares it with the prefix from the second data dictionary. If the names match and the data is not empty at that station, then it moves on to the next step - else it throws an error message to alert you of the emoty data. The next step is to ensure we are only calculating the metrics over common time steps,; the data is thus cropped to the later beginning and earlier end of the 2 datasets. The metrics are then stored with astring as the key and the value as the value, and the whole thing is stored in the dictionary that was intialised at the beginning of the function.

```{python}
#| eval: false
#| echo: true

def calculate_and_store_all_levels(dict1, dict2): 
    all_levels_metrics = {} # initialise emoty dictionary

    for key1, values1 in dict1.items():
      # the next 3 lines are to get the station name prefix out of the first dictionary
        split_parts = key1.split('_')
        station_name = split_parts[0]
        level_name = split_parts[2]
        matching_keys = [key2 for key2 in dict2.keys() if key2.startswith(station_name)]

        if matching_keys:
            for key2, values2 in dict2.items():
                if key2.startswith(station_name) and 'data' in values2:
                    x = np.array(values1.get('data', []))
                    y = np.array(values2.get('data', []))

                    if not (np.iterable(x) and np.iterable(y)):
                        print(f"Invalid data format for {key1} or {key2}")
                        continue

                    if len(x) == 0 or len(y) == 0:
                        print(f"Empty data array for {key1} or {key2}")
                        continue

                    min_len = min(len(x), len(y))
                    x = x[:min_len]
                    y = y[:min_len]

                    correlation_coefficient, r_squared, slope, intercept = calculate_regression_metrics(x, y)

                    level_metrics = {
                        'correlation_coefficient': correlation_coefficient,
                        'r_squared': r_squared,
                        'slope': slope,
                        'intercept': intercept
                    }

                    station_and_level = f"{station_name}_{level_name}"

                    if station_and_level not in all_levels_metrics:
                        all_levels_metrics[station_and_level] = {}

                    all_levels_metrics[station_and_level][station_name] = level_metrics

    return all_levels_metrics
```

I want to keep all my functions in one place, so I wont yet laod the results of the metric calculation; it will be found below, after the function definitions. 

If you want to save the results of all this work so far externally to a csv file, this is te function for you ! However, we wont do that here, since this is meant to be a standalone reproducible document. Here is the code chunk anyway: 

```{python}

#| eval: false
#| echo: true
def metrics_to_csv(metrics_dict, name1, name2):

    path = '/path/to/where/you/want/to/save/the/csv/'
    file_name = f"{name1}_{name2}_something_else.csv"
    file_path = path+file_name
    with open(file_path, 'w', newline='') as csv_file:
        writer = csv.writer(csv_file)

        header = ['Station', 'Correlation Coefficient', 'R-squared', 'Slope', 'Intercept']
        writer.writerow(header)


        for station in metrics_dict.keys():
            inner_dict = metrics_dict[station][next(iter(metrics_dict[station]))]
            row = [station, inner_dict['correlation_coefficient'], inner_dict['r_squared'], inner_dict['slope'], inner_dict['intercept']]
            writer.writerow(row)

    print(f'Successfully saved to {file_path}')
```
The following function plots the results of the above metric claculation visually, on an interactive world map. Please note that I had quite a few problems with the rendering of the images in html - it works fine in my local IDE. So I didnt use this code i n this report due to rendering issues, but it does work when used locally. 

```{python}
#| eval: false
#| echo: true
def create_correlation_map(df, metric='correlation'):
    fig = go.Figure()

    # scatter plot for stations
    scatter = go.Scattergeo(
        lon = df['lon'],
        lat = df['lat'],
        text = df.apply(lambda row: f"Station: {row['station']}<br>Level: {row['level']}<br>{metric}: {row[metric]:.3f}", axis=1),
        mode = 'markers',
        marker = dict(
            size = 8,
            color = df[metric],
            colorscale = 'RdBu',
            reversescale = True,
            colorbar_title = metric.capitalize(),
            cmin = -1 if metric == 'correlation' else df[metric].min(),
            cmax = 1 if metric == 'correlation' else df[metric].max(),
        )
    )
    fig.add_trace(scatter)

    # set the layout as desired
    fig.update_layout(
        title = dict(
            text = f'Map of {metric.capitalize()} between RECO and SWC',
            x = 0.5,  
            y = 0.95 
        ),
        geo = dict(
            showland = True,
            showcountries = True,
            showocean = True,
            countrywidth = 0.5,
            landcolor = 'rgb(243, 243, 243)',
            oceancolor = 'rgb(230, 230, 250)',
            projection = dict(
                type = 'natural earth',
                scale = 1.05  # slightly zoomed out
            ),
            lonaxis = dict(
                range = [-180, 180]  
            ),
            lataxis = dict(
                range = [-90, 90]  
            ),
        ),
        width = 1100,  
        height = 700, 
        margin = dict(l=20, r=20, t=30, b=20)  
    )

    return fig
```



As briefly mentioned above, the levels feature of the FLUXNET data is very interesting. So, we ave some functions to filter for that. First we need to check that the levels each have the same time step - it could be, that at a particular time step, levels 1,2, and 4 were recorded and not level 3, due to equipment failure for example. In this case, we do not want it, because we want a consistent time series. So we check first for this time step consistency:

```{python}
#| eval: false
#| echo: true
#| output: false

def remove_missing_time_steps(dict1):
    for station_name, levels_data in dict1.items():
        station_prefix = station_name.split('_')[0]

        level_keys = [key for key in dict1 if key.startswith(station_prefix)]

        if len(level_keys) < 2:
            continue

        # create a dictionary to store time data for each level
        time_data = {level: dict1[level]['time'] for level in level_keys}

        reference_level = level_keys[0]  # first level as the reference
        for current_level in level_keys[1:]:
            # Update time for the current level
            i = 0  # initialize index
            while i < len(time_data[current_level]):
                timestep = time_data[current_level][i]

                if timestep not in time_data[reference_level]:

                    # Remove corresponding 'data' value
                    del dict1[current_level]['data'][i]

                    # Remove the timestep from the list
                    del time_data[current_level][i]
                else:
                    i += 1  # move to the next timestep

            # Update dictionary with the modified time data
            dict1[current_level]['time'] = time_data[current_level]

    return dict1
``` 
Something I want to look at is some patterns between levels and stations, which involes retaining time steps at each station that have data for 4 levels only - not 3, or less. That is what this function is for - to refine the dictionaries to only include the data if it exists in fulll i.e. at all levels. 

```{python}
#| eval: false
#| echo: true

def four_levels_filter(data_dict, data_name):
    desired_levels_stations = {}

    for station_name in set(station.split('_')[0] for station in data_dict.keys()):
        # initialize a dictionary for the station
        station_data = {}

        for station in data_dict:
            if station.startswith(station_name):
                # extract level from the station name
                level = station.split('_')[-1][-2:]

                # get data and time_steps from the dictionary
                data = data_dict[station].get('data', [])
                time_steps = data_dict[station].get('time', [])

                # store data and time_steps in the station_data dictionary
                station_data[level] = {'data': data, 'time': time_steps}

        # check if the station has exactly 4 levels before storing in the dictionary
        if len(station_data) == 4:
            # Find common time steps among all levels
            common_time_steps = set.intersection(*[set(station_data[level]['time']) for level in station_data])

            # Filter data and time_steps to keep only common time steps
            for level in station_data:
                # Filter data and time_steps to keep only common time steps
                station_data[level]['data'] = [value for value, ts in zip(station_data[level]['data'], station_data[level]['time']) if ts in common_time_steps]
                station_data[level]['time'] = list(common_time_steps)

            desired_levels_stations[station_name] = station_data

            
 # Create a dictionary with the desired structure
    four_levels_only_gst_dict = {}
    
    for station_name, levels_data in desired_levels_stations.items():
        for level, data in levels_data.items():
            new_key = f"{station_name}_{data_name}_{level}"
            four_levels_only_gst_dict[new_key] = {'time': data['time'], 'data': data['data']}
    
    return four_levels_only_gst_dict
``` 

This function simply ensures that whichever 2 variables you want to play with cover the same period of time - if one i slonger than the other, the longer bit is removed by this function.
```{python}
#| eval: false
#| echo: true

def filter_level_data_common_time(dict1, dict2):    # order is important! dct1 is level data ie swc and dict2 is non leveled data ie reco
    dict1_filtered = {}
    dict2_filtered = {}

    for station_name, data1 in dict2.items():
        station_prefix = station_name.split('_')[0]
      #  print(station_name)
        for station_level_name, data2 in dict1.items():
            if station_level_name.startswith(station_prefix):
                level_name = station_level_name.split('_')[2] # the _l1_ etc bit of the name
                # print(level_name)
                #common_time_range = set(data1.get('time', [])).intersection(set(data2.get('time', [])))
                common_time_range = [time for time in data1.get('time', []) if time in data2.get('time', [])]

               # print(common_time_range)
            
                if common_time_range:

                    dict1_key = f"{station_level_name}"
                    dict1_filtered[dict1_key] = {'time': list(common_time_range), 'data': []}

                    dict1_filtered[dict1_key]['data'] = [value for time, value in zip(data2['time'], data2['data']) if time in common_time_range]
                    #print(len(dict1_filtered[dict1_key]['data']))

                    dict2_key = f"{station_name}_{level_name}"  # make new name for reco with level in it
                    dict2_filtered[dict2_key] = {'time': list(common_time_range), 'data': []}

                    dict2_filtered[dict2_key]['data'] = [value for time, value in zip(data1['time'], data1['data']) if time in common_time_range]
#                    print(len(dict2_filtered[dict2_key]['data']))

    return dict1_filtered, dict2_filtered
```





The following function creates an interactive plot of the seasonal variation of the data. The specific metrics plotted here are the mean, and the standard deviation. 

```{python}
#| eval: false
#| echo: false

# Function to create a seasonal plot for a station
def create_seasonal_plot(df, station):
    # Group by month and calculate mean and standard deviation
    monthly_data = df[df['station'] == station].groupby('month')['data'].agg(['mean', 'std']).reset_index()
    
    fig = go.Figure()
    
    # Add mean line
    fig.add_trace(go.Scatter(
        x=monthly_data['month'],
        y=monthly_data['mean'],
        mode='lines',
        name='Mean',
        line=dict(color='blue', width=2)
    ))
    
    # add shaded area for standard deviation
    fig.add_trace(go.Scatter(
        x=np.concatenate([monthly_data['month'], monthly_data['month'][::-1]]),
        y=np.concatenate([monthly_data['mean'] + monthly_data['std'], 
                          (monthly_data['mean'] - monthly_data['std'])[::-1]]),
        fill='toself',
        fillcolor='rgba(0,0,255,0.1)',
        line=dict(color='rgba(255,255,255,0)'),
        hoverinfo="skip",
        showlegend=False
    ))
    
    fig.update_layout(
        title=f'Average Seasonal Pattern for Station {station}',
        xaxis_title='Month',
        yaxis_title='SWC (%)',
        xaxis=dict(tickmode='array', tickvals=list(range(1, 13)), ticktext=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']),
        legend_title='',
        hovermode="x"
    )
    
    return fig

```


```{python}


# Function to create a seasonal plot for a station
def create_seasonal_plot(df, station):
    # Create a 2x2 subplot figure
    fig = make_subplots(rows=2, cols=2, subplot_titles=[f"Level {i+1}" for i in range(4)])
    
    station_data = df[df['station'] == station]
    
    for i, level in enumerate(station_data['level'].unique(), start=1):
        level_data = station_data[station_data['level'] == level]
        
        # Group by month and calculate mean and standard deviation
        monthly_data = level_data.groupby('month')['data'].agg(['mean', 'std']).reset_index()
        
        # Add mean line
        fig.add_trace(go.Scatter(
            x=monthly_data['month'],
            y=monthly_data['mean'],
            mode='lines',
            name=f'Level {level} Mean',
            line=dict(width=2)
        ), row=(i-1)//2 + 1, col=(i-1)%2 + 1)
        
        # Add shaded area for standard deviation
        fig.add_trace(go.Scatter(
            x=np.concatenate([monthly_data['month'], monthly_data['month'][::-1]]),
            y=np.concatenate([monthly_data['mean'] + monthly_data['std'], 
                              (monthly_data['mean'] - monthly_data['std'])[::-1]]),
            fill='toself',
            fillcolor='rgba(0,100,80,0.2)',
            line=dict(color='rgba(255,255,255,0)'),
            hoverinfo="skip",
            showlegend=False
        ), row=(i-1)//2 + 1, col=(i-1)%2 + 1)
    
    fig.update_layout(
        height=800,  # Adjust the height as needed
        width=800,  # Adjust the width as needed
        title_text=f'Seasonal Variation of SWC - Station {station}',
        showlegend=True
    )
    
    # Update x and y axis labels for all subplots
    for i in range(1, 3):
        for j in range(1, 3):
            fig.update_xaxes(title_text="Month", row=i, col=j, tickmode='array', tickvals=list(range(1, 13)), ticktext=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])
            fig.update_yaxes(title_text="SWC (%)", row=i, col=j)
    
    return fig

    


```

## Generating the results

So, that was a lot, but those functions make up most of the code! Now we can move on to some analysis. Lets use the 'process_data_files' function to stitch together our datasets, and also make a log version of the RECO data set too, since that could be interesting:

```{python}
reco_data_dict = process_data_files(reco_data)
log_reco_data_dict = log_transfo_data_dict(reco_data_dict)
swc_data_dict = process_data_files(soil_water_data)
```

First, we remove any inconsistencies in the levels data:

```{python}
#| output: false
# for retaining time steps with 4 levels only. so obvi do only with data that has levels (i.e. not reco!) 

remove_missing_time_steps(swc_data_dict) # if you want, you can name this and make it a seperate dictionary, to have one with 4 levels only, and one without the levels filtering. for te future of tis project, i am interested in the 4 levels only, so i will stick with that. 
```
Then we filter for the stations have have exclusively 4 levels over the whole time series.

```{python}
four_levels_swc = four_levels_filter(swc_data_dict, 'swc')
```

Then, we simply crop the 2 data sets we want to compare - here SWC and RECO, and SWC and logRECO, such that they have the same period.

```{python}
swc_filtered, reco_swc_filtered = filter_level_data_common_time(four_levels_swc, reco_data_dict)
swc_logfiltered, logreco_swc_filtered = filter_level_data_common_time(four_levels_swc, log_reco_data_dict)
```
Now, we calculate the regression metrics for this data.

```{python}
swc_reco_four_levels_metrics = calculate_and_store_all_levels(swc_filtered, reco_swc_filtered)
swc_four_levels_logreco_metrics = calculate_and_store_all_levels(swc_logfiltered, logreco_swc_filtered)
```


And if you wanted to save these metrics as a csv file, use this code chunk:

```{python}
#| eval: false
#| echo: true

metrics_to_csv(swc_reco_four_levels_metrics, 'SWC', 'RECO')
metrics_to_csv(swc_four_levels_logreco_metrics, 'SWC', 'logRECO')
```

So far, so cool! But its lacking a bit of explicit spatio temporal energy. So, lets firstly plot the stations on a world map so we can see where we are looking at. 


This is how I filtered the full station coordinate data set for only the ones that were present in my final datasets. You dont need to run this in the final reproducible workflow - but include but Ive included it as a snippet for full reproducibility.

```{python}

#| eval: false
#| echo: true
df_coords = pd.read_csv(station_location_csv)
station_names_from_dict = {key.split('_')[0] for key in swc_data_dict.keys()}
df_filtered = df_coords[df_coords['station'].isin(station_names_from_dict)]

# save the filtered DataFrame to a new CSV file
output_csv_file_path = '/pat/to/save/filtered/coord/data/
df_filtered.to_csv(output_csv_file_path, index=False)

print(f"Filtered CSV file has been saved to {output_csv_file_path}")
```

Using this freshly filtered dataset, lets plot the locations on a really nice, interactive plot. You can zoom in and out here, and hover over a point to see the name!

```{python}

# this is an interactive plot!! you can scroll and click!! and it tells you the name and coords of the station!!

# load the location file
df = pd.read_csv(filtered_station_locations)

# ensure latitude and longitude are numeric
df['lat'] = pd.to_numeric(df['lat'], errors='coerce')
df['lon'] = pd.to_numeric(df['lon'], errors='coerce')

# create the map
fig = go.Figure(data=go.Scattergeo(
    lon = df['lon'],
    lat = df['lat'],
    text = df['station'],  
    mode = 'markers',
    marker = dict(
        size = 5,
        color = 'red',
        line = dict(
            width = 1,
            color = 'rgba(68, 68, 68, 0)'
        )
    )
))

# layout for a world map with just outlines
fig.update_layout(
    title = dict(
        text = 'FLUXNET Station Locations',
        x = 0.5,  # this centres the title
        y = 0.95  # and this moves it down a little so its not touching the top
    ),
    geo = dict(
        showland = True,
        showcoastlines = True,
        showcountries = True,
        countrycolor = 'rgb(204, 204, 204)',
        coastlinecolor = 'rgb(204, 204, 204)',
        landcolor = 'rgb(255, 255, 255)',
        projection_type = 'natural earth',  # changed to natural earth projection - can also do orthographic, mercator, equirectangular...etc etc
        showocean = True,
        oceancolor = 'rgb(255, 255, 255)',
        showframe = False,
        lonaxis = dict(
            range = [-180, 180],  # crop longitude here if you dont want full map
        ),
        lataxis = dict(
            range = [-90, 90],  # same with latitude
        ),
    ),
    width = 1000,  # width of the figure
    height = 600,  # height of the figure
    margin = dict(l=0, r=0, t=30, b=0)  # reduce margins on sides
)

fig.show()
```

This map shows the locations ADD TO THIS . This is done for visualisation purposes. 


I now visualise the results of the metric calculations. The visual representation on a map creates an instructive first look at the potential patterns in the data. The full table of the regression metrics between the levels in SWC stations, and te RECO can be found in the following table. 

```{python}
# Load station locations
df = pd.read_csv(filtered_station_locations)



# Convert metrics dictionary to DataFrame
swc_reco_metrics_df = pd.DataFrame.from_dict(swc_reco_four_levels_metrics, orient='index')
swc_reco_metrics_df.reset_index(inplace=True)
swc_reco_metrics_df.rename(columns={'index': 'station'}, inplace=True)

# Extract location and level from station names in metrics_df
swc_reco_metrics_df[['location', 'level']] = metrics_df['station'].str.split('_', expand=True)

# Merge location data with your metrics
merged_df = swc_reco_metrics_df.merge(df, on='location', how='left')

def create_world_map(df, metric, level):
    level_data = df[df['level'] == level]
    
    fig = go.Figure(data=go.Scattergeo(
        lon = level_data['longitude'],
        lat = level_data['latitude'],
        text = level_data['station'] + '<br>' + metric + ': ' + level_data[metric].round(3).astype(str),
        mode = 'markers',
        marker = dict(
            size = 8,
            color = level_data[metric],
            colorscale = 'Viridis',
            showscale = True,
            colorbar_title = metric
        )
    ))

    fig.update_layout(
        title = f'{metric.capitalize()} for Level {level}',
        geo = dict(
            showland = True,
            showcountries = True,
            showocean = True,
            countrywidth = 0.5,
            landcolor = 'rgb(243, 243, 243)',
            oceancolor = 'rgb(230, 230, 250)',
        )
    )
    
    return fig

metrics = ['correlation', 'r_squared', 'slope', 'intercept']
levels = ['l1', 'l2', 'l3', 'l4']

for metric in metrics:
    for level in levels:
        fig = create_world_map(merged_df, metric, level)
        fig.show()
```

Here is a table of the results, too:

```{python}
numeric_columns = ['correlation', 'r_squared', 'slope', 'intercept']
metrics_df[numeric_columns] = metrics_df[numeric_columns].round(3)
metrics_df = metrics_df.sort_values('station')
print(metrics_df.to_markdown(index=False))
```


As can be seen from the output of this cell, some stations do not have location data. I am not really sure why this is; I checked again on the FLUXNET website, where the data is located, and they are not present there. Therefore the following world map visualisation of the data do not feature the stations that do not have available location information. The following code confirms which stations are not present in the location data table: 

```{python}
# checking which prefixes are present in the final dataset, just to see if I can Idenitfy tem. Taking into consideration the ones that dont have coordinates on FLUXNET, for some reason. 
unique_prefixes = set()

for key in swc_filtered.keys():
    # prefix extraction
    prefix = key.split('_')[0]
    unique_prefixes.add(prefix)

for prefix in unique_prefixes:
    print(prefix)
```

TALK A LITTLE ABOUT EACH OF THESE RESULTS

TEMPORAL EVOLUTION

We can now look at te temporal evolution of SWC and RECO at the different stations. This function filters for the stations with only continuous levels present, and plots them - with each plot featuring either 1,2,3, or 4 levels, depending on what was available in the data. The point is that the levels exist consistently across the time series. 




TALK A BIT ABOUT THE TEMP EVOL PLOTS
mention the lines in it

These plots are very messy, and a lot of them feature sections with very flat lines. These are simply points where, for a period of time, data at that level was not recorded for whatever reason. Even though this raw data is quite messy, we can still see 2 interesting things - the beaviour of SWC at different depths, and, in many cases, a strong seasonality. So then decided to look at the seasonal trends. Again, I wasnt able to create the plots in the way I wanted to for this (it would have been many many pages long), so I have included here 2 example plots. I chose the stations FR-Hes, and ANOTHER, since the have a long time series, and so are the most optimal for demonstrating seasonal variation with this dataset. However, it is not so bad, since they are 'just' visualisations, and the meaningful numbers can b


REDO THESE SEASNAL VARIATION PLOTS
DO THE SAME FOR RECO




## Outlook 

It would be great to find the locations of the statiosn that FLUXNET does not provide coordinate data for, such that there is more geographic data avilable to analyse. 

A next step for this project would be incorporating climatological data into the analysis - specifically precipitation and temperature - to better understand the relationship between SWC and RECO. Additionally, land cover classification for the stations could also be interesting to incorporate. 

