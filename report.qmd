---
title: "Soil Water Content and Ecosystem Respiration Analysis"
author: |
  **Eleanor Percy-Rouhaud**  
  Matriculation Number: 3749872
format:
  html:
    code-fold: true
    css: custom.css
jupyter: python3
execute:
  echo: true
  eval: true
  warning: false
---
```{python}
echo: False

from data_processing_functions import process_data_files, log_transfo_data_dict, metrics_to_csv, filter_level_data_common_time, calculate_regression_metrics, calculate_and_store_all_levels, remove_missing_time_steps, four_levels_filter

import os

# define  data paths
reco_data = './data/reco/'
soil_water_data = './data/swc/'
filtered_station_locations = './data/filtered-fluxnet-station-coords.csv'

# imported functions ready to use
reco_data_dict = process_data_files(reco_data)
log_reco_data_dict = log_transfo_data_dict(reco_data_dict)
swc_data_dict = process_data_files(soil_water_data)

# print("Data processing completed successfully")
```


## Introduction 

This is an analysis of Soil Water Content (SWC) and and Ecosystem Respiration (RECO) data. The variables are important for understanding ecosystem dynamics. SWC is a key component of the hydrological cycle, influencing water availability for plants and soil processes. It directly affects plant growth and productivity, . It also impacts soil structure, microbial activity, and organic matter decomposition. 

RECO is the total respiration from plants, animals, and microorganisms. It is a balance of carbon uptake (photosynthesis) and release (respiration). It can also be viewed as an indicator of ecosystem health. In this project, I am specifically interested in soil respiration, but unfortunately there isnt data I can use that measures only soil respiration, so Iwll start off by using RECO instead - which does include soil respiration, but also plant and animal respiration, too. 

We know already that there is an inflection point in the soil respiration with temperature - that is to say, both too high and too low temperatures cause soil respiration to decrease. Soil water content shows a similar beaviour. I want to investigate why - perhaps nitrogen content plays a role, or organic matter content. Wich is where RECO comes in .REPRASE THIS. 

Using data also from FLUXNET, the same exactly aanalysis I have done here can also be done with other climate variables, such as temperature, and ground heat flux. The process is the same as here, just with different variables. All this taken together will offer a new look into soil respiration and health. 

What makes this dataset in particular interesting is the levels in the SWC dataset. The SWC is measured at 4 different depts in the soil (suurface, 0.3m, 0.5m, and 1m below). While I didnt get too much time to look at this in this project, a goal is to look at the behaviour of SWC at 4 levels, and look for example for cross correlation in that data.  In this project, I did look at the regression metrics between RECO and SWC at different levels and different stations, to get a sense of their relationship. I also looked at the seasonal variation of the different stations and different levels. 

I chose also to look at the lograrithm of the RECO variable. WRITE WHY

This analysis aims to explore the relationship between SWC and RECO across various stations and levels. By examining these variables together, we can gain insights into how water availability influences carbon fluxes and overall ecosystem functioning.

## Setting up the functions to run the analysis

First, lets import the modules we will need to complete the analysis:

```{python}
# data access and manipulation modules
import pandas as pd 
import numpy as np
import csv
import math

# plotting libraries
import matplotlib.pyplot as plt
import plotly.graph_objects as go # interactive plots
from plotly.subplots import make_subplots # for dropdown menu options!
import matplotlib.dates as mdates
import tabulate # to make markdown formatted table
import datetime
from datetime import datetime
from matplotlib.backends.backend_pdf import PdfPages # saves multiple individual plots in 1 doc in case of not faceted plot
from collections import defaultdict # for plotting layers of each station
```

Now, lets access the data we will be analysing.

```{python}
reco_data = './data/reco/'
soil_water_data = './data/swc/'
filtered_station_locations = './data/filtered-fluxnet-station-coords.csv'
```

I wrote this code to also work with the many other variables and indices available from FLUXNET, so I wrote a lot of functions that can be used for many of them (rather than having to write a fresh code chunk each time). This is what now follows. Explanations of the function follows before each code block. 

The function below takes the datasets to be analysed, here SWC and RECO, and puts them into a more flexible form for the purposes of analysis and visualisation - a dictionary. The data directly from Fluxnet is in the form of individual .txt files for each station. Here, I take these individual files and all their contents, and stitch them together into a dictionary, to be accessed by the station name as a key.

```{python}
def process_data_files(directory):
    data_dict = {}
    for file in os.listdir(directory):
        file_path = os.path.join(directory, file)
        station_name = os.path.splitext(os.path.basename(file_path)).replace('_daily_avg', '')

        time = []
        data = []

        with open(file_path, 'r') as datafile:
            plotting = csv.reader(datafile, delimiter='\t')
            next(plotting)  # skips header row
            for ROWS in plotting:
                time.append(datetime.strptime(ROWS, '%Y-%m-%d'))
                data.append(float(ROWS))

        data_dict[station_name] = {'time': time, 'data': data}

    return data_dict
```

For some variables, it is interesting to take the log of its value; this is what this function does, and it creates a new dictionary with the same name + log, i.e., log_reco_data_dict.

```{python}
def log_transfo_data_dict(original_data_dict):
    log_transformed_dict = {}

    for station_name, data_info in original_data_dict.items():
        log_transformed_data = np.log(data_info['data'])
        log_transformed_dict[station_name] = {'time': data_info['time'], 'data': log_transformed_data}

    return log_transformed_dict
```

The function below takes whichever variables you want - again, here only SWC and RECO - and calculates various regression metrics of them, namely the R squared metric, the Pearson correlation coefficient, as well as the linear regression metrics (slope and intercept).

```{python}
def calculate_regression_metrics(x, y):
    x = np.array(x)
    y = np.array(y)

    # find indices where either x or y is NaN
    nan_indices = np.isnan(x) | np.isnan(y)

    # exclude NaN values from both x and y
    x_valid = x[~nan_indices]
    y_valid = y[~nan_indices]

    if len(x_valid) == 0 or len(y_valid) == 0:
        # if no valid data points are available, return NaN for all metrics
        return np.nan, np.nan, np.nan, np.nan

    correlation_coefficient = np.corrcoef(x_valid, y_valid)[0, 1]

    # linear regression for slope and intercept
    slope, intercept = np.polyfit(x_valid, y_valid, 1)

    residuals = y_valid - (slope * x_valid + intercept)
    ss_residual = np.sum(residuals**2)
    ss_total = np.sum((y_valid - np.mean(y_valid))**2)
    r_squared = 1 - (ss_residual / ss_total)

    return correlation_coefficient, r_squared, slope, intercept
```

Once the metrics are calculated, it is necessary to have them stored somewhere - so here is a function to store them in a dictionary. It extracts the prefix of the station for the first data dictionary, which indicates its name, and compares it with the prefix from the second data dictionary. If the names match and the data is not empty at that station, then it moves on to the next step - else it throws an error message to alert you of the empty data. The next step is to ensure we are only calculating the metrics over common time steps; the data is thus cropped to the later beginning and earlier end of the two datasets. The metrics are then stored with a string as the key and the value as the value, and the whole thing is stored in the dictionary that was initialised at the beginning of the function.

```{python}
#| code-fold: true

def calculate_and_store_all_levels(dict1, dict2): 
    all_levels_metrics = {}  # initialise empty dictionary

    for key1, values1 in dict1.items():
        # the next 3 lines are to get the station name prefix out of the first dictionary
        split_parts = key1.split('_')
        station_name = split_parts
        level_name = split_parts
        matching_keys = [key2 for key2 in dict2.keys() if key2.startswith(station_name)]

        if matching_keys:
            for key2, values2 in dict2.items():
                if key2.startswith(station_name) and 'data' in values2:
                    x = np.array(values1.get('data', []))
                    y = np.array(values2.get('data', []))

                    if not (np.iterable(x) and np.iterable(y)):
                        print(f"Invalid data format for {key1} or {key2}")
                        continue

                    if len(x) == 0 or len(y) == 0:
                        print(f"Empty data array for {key1} or {key2}")
                        continue

                    min_len = min(len(x), len(y))
                    x = x[:min_len]
                    y = y[:min_len]

                    correlation_coefficient, r_squared, slope, intercept = calculate_regression_metrics(x, y)

                    level_metrics = {
                        'correlation_coefficient': correlation_coefficient,
                        'r_squared': r_squared,
                        'slope': slope,
                        'intercept': intercept
                    }

                    station_and_level = f"{station_name}_{level_name}"

                    if station_and_level not in all_levels_metrics:
                        all_levels_metrics[station_and_level] = {}

                    all_levels_metrics[station_and_level][station_name] = level_metrics

    return all_levels_metrics
```

I want to keep all my functions in one place, so I wont yet laod the results of the metric calculation; it will be found below, after the function definitions. 

If you want to save the results of all this work so far externally to a csv file, this is te function for you ! However, we wont do that here, since this is meant to be a standalone reproducible document. Here is the code chunk anyway: 

```{python}
def metrics_to_csv(metrics_dict, name1, name2):

    path = '/path/to/where/you/want/to/save/the/csv/'
    file_name = f"{name1}_{name2}_something_else.csv"
    file_path = path+file_name
    with open(file_path, 'w', newline='') as csv_file:
        writer = csv.writer(csv_file)

        header = ['Station', 'Correlation Coefficient', 'R-squared', 'Slope', 'Intercept']
        writer.writerow(header)


        for station in metrics_dict.keys():
            inner_dict = metrics_dict[station][next(iter(metrics_dict[station]))]
            row = [station, inner_dict['correlation_coefficient'], inner_dict['r_squared'], inner_dict['slope'], inner_dict['intercept']]
            writer.writerow(row)

    print(f'Successfully saved to {file_path}')
```
The following function plots the results of the above metric claculation visually, on an interactive world map. Please note that I had quite a few problems with the rendering of the images in html - it works fine in my local IDE. So I didnt use this code i n this report due to rendering issues, but it does work when used locally. 

```{python}
def create_correlation_map(df, metric='correlation'):
    fig = go.Figure()

    # scatter plot for stations
    scatter = go.Scattergeo(
        lon = df['lon'],
        lat = df['lat'],
        text = df.apply(lambda row: f"Station: {row['station']}<br>Level: {row['level']}<br>{metric}: {row[metric]:.3f}", axis=1),
        mode = 'markers',
        marker = dict(
            size = 8,
            color = df[metric],
            colorscale = 'RdBu',
            reversescale = True,
            colorbar_title = metric.capitalize(),
            cmin = -1 if metric == 'correlation' else df[metric].min(),
            cmax = 1 if metric == 'correlation' else df[metric].max(),
        )
    )
    fig.add_trace(scatter)

    # set the layout as desired
    fig.update_layout(
        title = dict(
            text = f'Map of {metric.capitalize()} between RECO and SWC',
            x = 0.5,  
            y = 0.95 
        ),
        geo = dict(
            showland = True,
            showcountries = True,
            showocean = True,
            countrywidth = 0.5,
            landcolor = 'rgb(243, 243, 243)',
            oceancolor = 'rgb(230, 230, 250)',
            projection = dict(
                type = 'natural earth',
                scale = 1.05  # slightly zoomed out
            ),
            lonaxis = dict(
                range = [-180, 180]  
            ),
            lataxis = dict(
                range = [-90, 90]  
            ),
        ),
        width = 1100,  
        height = 700, 
        margin = dict(l=20, r=20, t=30, b=20)  
    )

    return fig
```


As briefly mentioned above, the levels feature of the FLUXNET data is very interesting. So, we ave some functions to filter for that. First we need to check that the levels each have the same time step - it could be, that at a particular time step, levels 1,2, and 4 were recorded and not level 3, due to equipment failure for example. In this case, we do not want it, because we want a consistent time series. So we check first for this time step consistency:

```{python}
#| eval: false
#| echo: true
#| output: false

def remove_missing_time_steps(dict1):
    for station_name, levels_data in dict1.items():
        station_prefix = station_name.split('_')[0]

        level_keys = [key for key in dict1 if key.startswith(station_prefix)]

        if len(level_keys) < 2:
            continue

        # create a dictionary to store time data for each level
        time_data = {level: dict1[level]['time'] for level in level_keys}

        reference_level = level_keys[0]  # first level as the reference
        for current_level in level_keys[1:]:
            # Update time for the current level
            i = 0  # initialize index
            while i < len(time_data[current_level]):
                timestep = time_data[current_level][i]

                if timestep not in time_data[reference_level]:

                    # Remove corresponding 'data' value
                    del dict1[current_level]['data'][i]

                    # Remove the timestep from the list
                    del time_data[current_level][i]
                else:
                    i += 1  # move to the next timestep

            # Update dictionary with the modified time data
            dict1[current_level]['time'] = time_data[current_level]

    return dict1
``` 
Something I want to look at is some patterns between levels and stations, which involes retaining time steps at each station that have data for 4 levels only - not 3, or less. That is what this function is for - to refine the dictionaries to only include the data if it exists in fulll i.e. at all levels. 

```{python}
#| eval: false
#| echo: true

def four_levels_filter(data_dict, data_name):
    desired_levels_stations = {}

    for station_name in set(station.split('_')[0] for station in data_dict.keys()):
        # initialize a dictionary for the station
        station_data = {}

        for station in data_dict:
            if station.startswith(station_name):
                # extract level from the station name
                level = station.split('_')[-1][-2:]

                # get data and time_steps from the dictionary
                data = data_dict[station].get('data', [])
                time_steps = data_dict[station].get('time', [])

                # store data and time_steps in the station_data dictionary
                station_data[level] = {'data': data, 'time': time_steps}

        # check if the station has exactly 4 levels before storing in the dictionary
        if len(station_data) == 4:
            # Find common time steps among all levels
            common_time_steps = set.intersection(*[set(station_data[level]['time']) for level in station_data])

            # Filter data and time_steps to keep only common time steps
            for level in station_data:
                # Filter data and time_steps to keep only common time steps
                station_data[level]['data'] = [value for value, ts in zip(station_data[level]['data'], station_data[level]['time']) if ts in common_time_steps]
                station_data[level]['time'] = list(common_time_steps)

            desired_levels_stations[station_name] = station_data

            
 # Create a dictionary with the desired structure
    four_levels_only_gst_dict = {}
    
    for station_name, levels_data in desired_levels_stations.items():
        for level, data in levels_data.items():
            new_key = f"{station_name}_{data_name}_{level}"
            four_levels_only_gst_dict[new_key] = {'time': data['time'], 'data': data['data']}
    
    return four_levels_only_gst_dict
``` 

This function simply ensures that whichever 2 variables you want to play with cover the same period of time - if one i slonger than the other, the longer bit is removed by this function.
```{python}
#| eval: false
#| echo: true

def filter_level_data_common_time(dict1, dict2):    # order is important! dct1 is level data ie swc and dict2 is non leveled data ie reco
    dict1_filtered = {}
    dict2_filtered = {}

    for station_name, data1 in dict2.items():
        station_prefix = station_name.split('_')[0]
      #  print(station_name)
        for station_level_name, data2 in dict1.items():
            if station_level_name.startswith(station_prefix):
                level_name = station_level_name.split('_')[2] # the _l1_ etc bit of the name
                # print(level_name)
                #common_time_range = set(data1.get('time', [])).intersection(set(data2.get('time', [])))
                common_time_range = [time for time in data1.get('time', []) if time in data2.get('time', [])]

               # print(common_time_range)
            
                if common_time_range:

                    dict1_key = f"{station_level_name}"
                    dict1_filtered[dict1_key] = {'time': list(common_time_range), 'data': []}

                    dict1_filtered[dict1_key]['data'] = [value for time, value in zip(data2['time'], data2['data']) if time in common_time_range]
                    #print(len(dict1_filtered[dict1_key]['data']))

                    dict2_key = f"{station_name}_{level_name}"  # make new name for reco with level in it
                    dict2_filtered[dict2_key] = {'time': list(common_time_range), 'data': []}

                    dict2_filtered[dict2_key]['data'] = [value for time, value in zip(data1['time'], data1['data']) if time in common_time_range]
#                    print(len(dict2_filtered[dict2_key]['data']))

    return dict1_filtered, dict2_filtered
```





The following function creates an interactive plot of the seasonal variation of the data. The specific metrics plotted here are the mean, and the standard deviation. 

```{python}
#| eval: false
#| echo: false

# Function to create a seasonal plot for a station
def create_seasonal_plot(df, station):
    # Group by month and calculate mean and standard deviation
    monthly_data = df[df['station'] == station].groupby('month')['data'].agg(['mean', 'std']).reset_index()
    
    fig = go.Figure()
    
    # Add mean line
    fig.add_trace(go.Scatter(
        x=monthly_data['month'],
        y=monthly_data['mean'],
        mode='lines',
        name='Mean',
        line=dict(color='blue', width=2)
    ))
    
    # add shaded area for standard deviation
    fig.add_trace(go.Scatter(
        x=np.concatenate([monthly_data['month'], monthly_data['month'][::-1]]),
        y=np.concatenate([monthly_data['mean'] + monthly_data['std'], 
                          (monthly_data['mean'] - monthly_data['std'])[::-1]]),
        fill='toself',
        fillcolor='rgba(0,0,255,0.1)',
        line=dict(color='rgba(255,255,255,0)'),
        hoverinfo="skip",
        showlegend=False
    ))
    
    fig.update_layout(
        title=f'Average Seasonal Pattern for Station {station}',
        xaxis_title='Month',
        yaxis_title='SWC (%)',
        xaxis=dict(tickmode='array', tickvals=list(range(1, 13)), ticktext=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']),
        legend_title='',
        hovermode="x"
    )
    
    return fig

```


```{python}


# Function to create a seasonal plot for a station
def create_seasonal_plot(df, station):
    # Create a 2x2 subplot figure
    fig = make_subplots(rows=2, cols=2, subplot_titles=[f"Level {i+1}" for i in range(4)])
    
    station_data = df[df['station'] == station]
    
    for i, level in enumerate(station_data['level'].unique(), start=1):
        level_data = station_data[station_data['level'] == level]
        
        # Group by month and calculate mean and standard deviation
        monthly_data = level_data.groupby('month')['data'].agg(['mean', 'std']).reset_index()
        
        # Add mean line
        fig.add_trace(go.Scatter(
            x=monthly_data['month'],
            y=monthly_data['mean'],
            mode='lines',
            name=f'Level {level} Mean',
            line=dict(width=2)
        ), row=(i-1)//2 + 1, col=(i-1)%2 + 1)
        
        # Add shaded area for standard deviation
        fig.add_trace(go.Scatter(
            x=np.concatenate([monthly_data['month'], monthly_data['month'][::-1]]),
            y=np.concatenate([monthly_data['mean'] + monthly_data['std'], 
                              (monthly_data['mean'] - monthly_data['std'])[::-1]]),
            fill='toself',
            fillcolor='rgba(0,100,80,0.2)',
            line=dict(color='rgba(255,255,255,0)'),
            hoverinfo="skip",
            showlegend=False
        ), row=(i-1)//2 + 1, col=(i-1)%2 + 1)
    
    fig.update_layout(
        height=800,  # Adjust the height as needed
        width=800,  # Adjust the width as needed
        title_text=f'Seasonal Variation of SWC - Station {station}',
        showlegend=True
    )
    
    # Update x and y axis labels for all subplots
    for i in range(1, 3):
        for j in range(1, 3):
            fig.update_xaxes(title_text="Month", row=i, col=j, tickmode='array', tickvals=list(range(1, 13)), ticktext=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])
            fig.update_yaxes(title_text="SWC (%)", row=i, col=j)
    
    return fig

    


```

## Generating the results

So, that was a lot, but those functions make up most of the code! Now we can move on to some analysis. Lets use the 'process_data_files' function to stitch together our datasets, and also make a log version of the RECO data set too, since that could be interesting:

```{python}
reco_data_dict = process_data_files(reco_data)
log_reco_data_dict = log_transfo_data_dict(reco_data_dict)
swc_data_dict = process_data_files(soil_water_data)
```

First, we remove any inconsistencies in the levels data:

```{python}
#| output: false
# for retaining time steps with 4 levels only. so obvi do only with data that has levels (i.e. not reco!) 

remove_missing_time_steps(swc_data_dict) # if you want, you can name this and make it a seperate dictionary, to have one with 4 levels only, and one without the levels filtering. for te future of tis project, i am interested in the 4 levels only, so i will stick with that. 
```
Then we filter for the stations have have exclusively 4 levels over the whole time series.

```{python}
four_levels_swc = four_levels_filter(swc_data_dict, 'swc')
```

Then, we simply crop the 2 data sets we want to compare - here SWC and RECO, and SWC and logRECO, such that they have the same period.

```{python}
swc_filtered, reco_swc_filtered = filter_level_data_common_time(four_levels_swc, reco_data_dict)
swc_logfiltered, logreco_swc_filtered = filter_level_data_common_time(four_levels_swc, log_reco_data_dict)
```
Now, we calculate the regression metrics for this data.

```{python}
swc_reco_four_levels_metrics = calculate_and_store_all_levels(swc_filtered, reco_swc_filtered)
swc_four_levels_logreco_metrics = calculate_and_store_all_levels(swc_logfiltered, logreco_swc_filtered)
```


And if you wanted to save these metrics as a csv file, use this code chunk:

```{python}
#| eval: false
#| echo: true

metrics_to_csv(swc_reco_four_levels_metrics, 'SWC', 'RECO')
metrics_to_csv(swc_four_levels_logreco_metrics, 'SWC', 'logRECO')
```

So far, so cool! But its lacking a bit of explicit spatio temporal energy. So, lets firstly plot the stations on a world map so we can see where we are looking at. 


This is how I filtered the full station coordinate data set for only the ones that were present in my final datasets. You dont need to run this in the final reproducible workflow - but include but Ive included it as a snippet for full reproducibility.

```{python}

#| eval: false
#| echo: true
df_coords = pd.read_csv(station_location_csv)
station_names_from_dict = {key.split('_')[0] for key in swc_data_dict.keys()}
df_filtered = df_coords[df_coords['station'].isin(station_names_from_dict)]

# save the filtered DataFrame to a new CSV file
output_csv_file_path = '/pat/to/save/filtered/coord/data/
df_filtered.to_csv(output_csv_file_path, index=False)

print(f"Filtered CSV file has been saved to {output_csv_file_path}")
```

Here are all the possible stations we could look at with the datasets available:

```{python}

# this is an interactive plot!! you can scroll and click!! and it tells you the name and coords of the station!!

# load the location file
df = pd.read_csv(filtered_station_locations)

# ensure latitude and longitude are numeric
df['lat'] = pd.to_numeric(df['lat'], errors='coerce')
df['lon'] = pd.to_numeric(df['lon'], errors='coerce')

# create the map
fig = go.Figure(data=go.Scattergeo(
    lon = df['lon'],
    lat = df['lat'],
    text = df['station'],  
    mode = 'markers',
    marker = dict(
        size = 5,
        color = 'red',
        line = dict(
            width = 1,
            color = 'rgba(68, 68, 68, 0)'
        )
    )
))

# layout for a world map with just outlines
fig.update_layout(
    title = dict(
        text = 'FLUXNET Station Locations (all levels present in dataset)',
        x = 0.5,  # this centres the title
        y = 0.95  # and this moves it down a little so its not touching the top
    ),
    geo = dict(
        showland = True,
        showcoastlines = True,
        showcountries = True,
        countrycolor = 'rgb(204, 204, 204)',
        coastlinecolor = 'rgb(204, 204, 204)',
        landcolor = 'rgb(255, 255, 255)',
        projection_type = 'natural earth',  # changed to natural earth projection - can also do orthographic, mercator, equirectangular...etc etc
        showocean = True,
        oceancolor = 'rgb(255, 255, 255)',
        showframe = False,
        lonaxis = dict(
            range = [-180, 180],  # crop longitude here if you dont want full map
        ),
        lataxis = dict(
            range = [-90, 90],  # same with latitude
        ),
    ),
    width = 1000,  # width of the figure
    height = 600,  # height of the figure
    margin = dict(l=0, r=0, t=30, b=0)  # reduce margins on sides
)

fig.show()
```
So it filtered for the stations present in the data, now I want to filter for stations with only 4 levels. I kept them as seperate datasets, and didn't go straight in for this filtering because I also need at other points in this project (not displayed in this report)  the data for all the stations, not just those with 4 levels of data. 

Using this freshly filtered dataset, lets plot the locations on a really nice, interactive plot. You can zoom in and out here, and hover over a point to see the name!
```{python}

# Extract unique station names from swc_filtered keys
stations_in_swc = set(key.split('_')[0] for key in swc_filtered.keys())

# Load the location file
df = pd.read_csv(filtered_station_locations)

# Filter the dataframe to include only the stations in swc_filtered
df_filtered = df[df['station'].isin(stations_in_swc)]

# Ensure latitude and longitude are numeric
df_filtered['lat'] = pd.to_numeric(df_filtered['lat'], errors='coerce')
df_filtered['lon'] = pd.to_numeric(df_filtered['lon'], errors='coerce')

# Create the map
fig = go.Figure(data=go.Scattergeo(
    lon = df_filtered['lon'],
    lat = df_filtered['lat'],
    text = df_filtered['station'],  
    mode = 'markers',
    marker = dict(
        size = 8,  # Increased size for better visibility
        color = 'red',
        line = dict(
            width = 1,
            color = 'rgba(68, 68, 68, 0)'
        )
    )
))

# Layout for a world map with just outlines
fig.update_layout(
    title = dict(
        text = 'FLUXNET Station Locations (where only 4 levels are present)',
        x = 0.5,
        y = 0.95
    ),
    geo = dict(
        showland = True,
        showcoastlines = True,
        showcountries = True,
        countrycolor = 'rgb(204, 204, 204)',
        coastlinecolor = 'rgb(204, 204, 204)',
        landcolor = 'rgb(255, 255, 255)',
        projection_type = 'natural earth',
        showocean = True,
        oceancolor = 'rgb(255, 255, 255)',
        showframe = False,
        lonaxis = dict(
            range = [-180, 180],
        ),
        lataxis = dict(
            range = [-90, 90],
        ),
    ),
    width = 1000,
    height = 600,
    margin = dict(l=0, r=0, t=30, b=0)
)

fig.show()
```

This map shows the locations ADD TO THIS . This is done for visualisation purposes. 


I now visualise the results of the metric calculations. The visual representation on a map creates an instructive first look at the potential patterns in the data. Ive displayed it as a dropdown menu, to save some space on the page, and to easily find the variable one might be interested in. 

```{python}

def create_world_correlation_map(df, metrics, levels, data_type): # for now, ave to manually type in the variable for the title. want to imrove this to be dynamic later.
    fig = make_subplots(rows=1, cols=1, specs=[[{'type': 'scattergeo'}]])

    # Create a base layer with all data points
    fig.add_trace(go.Scattergeo(
        lon = df['lon'],
        lat = df['lat'],
        mode = 'markers',
        marker = dict(size = 8, color = 'lightgrey'),
        showlegend = False
    ))

    # Create traces for each combination of metric and level
    for metric in metrics:
        for level in levels:
            level_data = df[df['level'] == level]
            fig.add_trace(go.Scattergeo(
                lon = level_data['lon'],
                lat = level_data['lat'],
                text = level_data['station'] + '<br>' + metric + ': ' + level_data[metric].round(3).astype(str),
                mode = 'markers',
                marker = dict(
                    size = 8,
                    color = level_data[metric],
                    colorscale = 'Viridis',
                    showscale = True,
                  #  colorbar_title = metric
                ),
                name = f'{metric} - Level {level}',
                visible = False  # Start with all traces hidden
            ))

    # Make the first trace visible
    fig.data[1].visible = True

    # Create dropdown menu
    dropdown_buttons = []
    for i, metric in enumerate(metrics):
        for j, level in enumerate(levels):
            visible = [False] * len(fig.data)
            visible[0] = True  # Base layer always visible
            visible[1 + i*len(levels) + j] = True  # Make the selected trace visible
            dropdown_buttons.append(
                dict(
                    label = f'{metric.capitalize()} - Level {level}',
                    method = 'update',
                    args = [{'visible': visible},
                            {'annotations[1].text': f'{metric.capitalize()} between SWC and {data_type} for Level {level}, all stations'}]
                )
            )

    # Set the initial title
    initial_title = f'{metrics[0].capitalize()} between SWC and {data_type} for Level {levels[0]}, all stations'

    fig.update_layout(
        updatemenus=[dict(
            active=0,
            buttons=dropdown_buttons,
            direction="down",
            pad={"r": 10, "t": 10},
            showactive=True,
            x=1,
            xanchor="right",
            y=1.15,  # Position the dropdown menu
            yanchor="top"
        )],
        geo=dict(
            showland=True,
            showcountries=True,
            showocean=True,
            countrywidth=0.5,
            landcolor='rgb(243, 243, 243)',
            oceancolor='rgb(230, 230, 250)',
            projection_type='natural earth',
            showcoastlines=True,
            coastlinecolor="RebeccaPurple",
            showframe=False,
            lonaxis=dict(showgrid=False),
            lataxis=dict(showgrid=False)
        ),
        autosize=False,
        width=900,
        height=600,  # Increased height to accommodate title and dropdown
        margin=dict(l=0, r=0, t=150, b=0)  # Increased top margin for title and dropdown
    )
    
    # Add annotations for dropdown label and title
    fig.add_annotation(
        text="Select Metric and Level:",
        xref="paper", yref="paper",
        x=0.3, y=1.12,  # Position for dropdown label
        showarrow=False,
        font=dict(size=14)
    )
    
    fig.add_annotation(
        text=initial_title,
        xref="paper", yref="paper",
        x=0.5, y=1.06,  # Position for title
        showarrow=False,
        font=dict(size=16, color="black"),
        align="center"
    )
    
    return fig
# Load station locations
station_locations_df = pd.read_csv(filtered_station_locations)


# Transform the dictionary into a DataFrame
data = []
for key, value in swc_reco_four_levels_metrics.items():
    level = key.split('_')[1]
    for station, metrics in value.items():
        row = {'station': station, 'level': level}
        row.update(metrics)
        data.append(row)

swc_reco_metrics_df = pd.DataFrame(data)


# Merge location data with your metrics based on the station name
swc_reco_merged_df = swc_reco_metrics_df.merge(station_locations_df, on='station', how='left')

# Create the figure
metrics = ['correlation_coefficient', 'r_squared', 'slope', 'intercept']
levels = ['l1', 'l2', 'l3', 'l4']

fig = create_world_correlation_map(swc_reco_merged_df, metrics, levels, 'RECO')
fig.show()

```
And here is the same plot but with the logRECO and SWC regression calculations: 
```{python}

# Load station locations
station_locations_df = pd.read_csv(filtered_station_locations)


# Transform the dictionary into a DataFrame
data = []
for key, value in swc_four_levels_logreco_metrics.items():
    level = key.split('_')[1]
    for station, metrics in value.items():
        row = {'station': station, 'level': level}
        row.update(metrics)
        data.append(row)

swc_logreco_metrics_df = pd.DataFrame(data)


# Merge location data with your metrics based on the station name
swc_logreco_merged_df = swc_logreco_metrics_df.merge(station_locations_df, on='station', how='left')



# Create the figure
metrics = ['correlation_coefficient', 'r_squared', 'slope', 'intercept']
levels = ['l1', 'l2', 'l3', 'l4']

fig = create_world_correlation_map(swc_logreco_merged_df, metrics, levels, 'logRECO')
fig.show()
```

Here is a table of the results of the regression metrics between SWC and RECO, too:

```{python}
level_order = ['l1', 'l2', 'l3', 'l4']

swc_reco_metrics_df['level'] = pd.Categorical(swc_reco_metrics_df['level'], categories=level_order, ordered=True) # oterwise prints the table in a really topsy turrvy way
swc_reco_metrics_df = swc_reco_metrics_df.sort_values(['station', 'level'])

numeric_columns = ['correlation_coefficient', 'r_squared', 'slope', 'intercept']
swc_reco_metrics_df[numeric_columns] = swc_reco_metrics_df[numeric_columns].round(3)

markdown_table = swc_reco_metrics_df.to_markdown(index=False)

print(markdown_table)

```

And the same for SWC and logRECO:

```{python}
swc_logreco_metrics_df['level'] = pd.Categorical(swc_logreco_metrics_df['level'], categories=level_order, ordered=True) # oterwise prints the table in a really topsy turrvy way
swc_logreco_metrics_df = swc_logreco_metrics_df.sort_values(['station', 'level'])

numeric_columns = ['correlation_coefficient', 'r_squared', 'slope', 'intercept']
swc_logreco_metrics_df[numeric_columns] = swc_logreco_metrics_df[numeric_columns].round(3)

markdown_table = swc_logreco_metrics_df.to_markdown(index=False)

print(markdown_table)
```


As can be seen from the output of this cell, some stations do not have location data. I am not really sure why this is; I checked again on the FLUXNET website, where the data is located, and they are not present there. Therefore the following world map visualisation of the data do not feature the stations that do not have available location information. The following code confirms which stations are not present in the location data table: 

```{python}
# checking which prefixes are present in the final dataset, just to see if I can Idenitfy tem. Taking into consideration the ones that dont have coordinates on FLUXNET, for some reason. 
unique_prefixes = set()

for key in swc_filtered.keys():
    # prefix extraction
    prefix = key.split('_')[0]
    unique_prefixes.add(prefix)

for prefix in unique_prefixes:
    print(prefix)
```

TALK A LITTLE ABOUT EACH OF THESE RESULTS

TEMPORAL EVOLUTION

We can now look at te temporal evolution of SWC and RECO at the different stations. Here I am plotting only the stations I filtered for earlier with 4 levels. Since the dictionaries I use to structure the data have the same structure, you could also use the non filtered data (swc_data_dict, for eaxmple) to see more stations. 

Again, this is an interactive plot set up featuring a drop down menu, for space saving reasons. 

```{python}
def create_timeseries_plot(data, data_type):
    # Extract unique stations from the keys
    if data_type == 'swc':
        stations = list(set([key.split('_')[0] for key in data.keys()]))
    else:
        stations = list(data.keys())
    
    # Create the figure
    fig = go.Figure()

    # Create traces for each station (initially hidden)
    for station in stations:
        if data_type == 'swc':
            for level in ['l1', 'l2', 'l3', 'l4']:
                key = f"{station}_{data_type}_{level}"
                if key in data:
                    fig.add_trace(go.Scatter(
                        x=data[key]['time'],
                        y=data[key]['data'],
                        mode='lines',
                        name=f'{level}',
                        visible=False
                    ))
        else:
            fig.add_trace(go.Scatter(
                x=data[station]['time'],
                y=data[station]['data'],
                mode='lines',
                name=station,
                visible=False
            ))

    # Create and add dropdown menu
    dropdown_buttons = []
    for i, station in enumerate(stations):
        visible = [False] * len(fig.data)
        if data_type == 'swc':
            start_idx = i * 4
            for j in range(4):  # 4 levels per station
                if start_idx + j < len(fig.data):
                    visible[start_idx + j] = True
        else:
            visible[i] = True
        
        dropdown_buttons.append(dict(
            label=station,
            method='update',
            args=[{'visible': visible},
                  {'title': f'{data_type.upper()} Time Series for Station {station}'}]
        ))

    fig.update_layout(
        updatemenus=[dict(
            active=0,
            buttons=dropdown_buttons,
            direction="down",
            pad={"r": 10, "t": 10},
            showactive=True,
            x=0.1,
            xanchor="left",
            y=1.15,
            yanchor="top"
        )],
        height=600,
        title_text=f"Select a Station to view {data_type.upper()} Time Series",
        title_x=0.5,
        xaxis_title="Time",
        yaxis_title=data_type.upper()
    )

    # Make the first station visible by default
    if data_type == 'swc':
        for i in range(min(4, len(fig.data))):
            fig.data[i].visible = True
    else:
        fig.data[0].visible = True

    return fig
# Create plots for each dataset
create_timeseries_plot(swc_filtered, 'swc')
```
And here is the RECO time series: 

```{python}
#| eval: true
#| echo: false
create_timeseries_plot(reco_swc_filtered, 'reco')
```

I recommend looking at DE-HoH for a nice example of a a clean plot with the 4 levels well represented. 

TALK A BIT ABOUT THE TEMP EVOL PLOTS
mention the lines in it

These plots are very messy, and a lot of them feature sections with very flat lines. These are simply points where, for a period of time, data at that level was not recorded for whatever reason. Even though this raw data is quite messy, we can still see 2 interesting things - the beaviour of SWC at different depths, and, in many cases, a strong seasonality. So then decided to look at the seasonal trends. Again, I wasnt able to create the plots in the way I wanted to for this (it would have been many many pages long), so I have included here 2 example plots. I chose the stations FR-Hes, and ANOTHER, since the have a long time series, and so are the most optimal for demonstrating seasonal variation with this dataset. However, it is not so bad, since they are 'just' visualisations, and the meaningful numbers can b

DESCRIBE WHAT THE FUNCTION DOES

```{python}
#| eval: true
#| echo: false
month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']

def create_seasonal_plot_multi_level(data, data_type):
    stations = list(set([key.split('_')[0] for key in data.keys()]))
    
    fig = make_subplots(rows=4, cols=1, subplot_titles=("Level 1", "Level 2", "Level 3", "Level 4"),
                        shared_xaxes=True, vertical_spacing=0.05)
    levels = ['l1', 'l2', 'l3', 'l4']

    for station in stations:
        for i, level in enumerate(levels):
            key = f"{station}_{data_type}_{level}"
            if key in data:
                df = pd.DataFrame({
                    'time': pd.to_datetime(data[key]['time']),
                    'value': data[key]['data']
                })
                df['month'] = df['time'].dt.month
                df = df.dropna(subset=['value'])

                monthly_stats = df.groupby('month')['value'].agg(['mean', 'std']).reset_index()

                # Plot mean line
                fig.add_trace(go.Scatter(
                    x=monthly_stats['month'],
                    y=monthly_stats['mean'],
                    mode='lines+markers',
                    name=f'{station} - Mean',
                    line=dict(color='blue', width=2),
                    visible=False
                ), row=i+1, col=1)

                # Add error bars
                fig.add_trace(go.Scatter(
                    x=monthly_stats['month'],
                    y=monthly_stats['mean'],
                    error_y=dict(
                        type='data',
                        array=monthly_stats['std'],
                        visible=True
                    ),
                    mode='markers',
                    marker=dict(color='red', size=8),
                    name=f'{station} - Std Dev',
                    showlegend=False,
                    visible=False
                ), row=i+1, col=1)

    # Create dropdown menu
    dropdown_buttons = []
    for i, station in enumerate(stations):
        visible = [False] * len(fig.data)
        for j in range(8):  # 4 levels * 2 traces per level
            if i*8 + j < len(fig.data):
                visible[i*8 + j] = True
        
        dropdown_buttons.append(dict(
            label=station,
            method='update',
            args=[{'visible': visible},
                  {'title': f'Seasonal Variation of {data_type.upper()} for Station {station}'}]
        ))

    fig.update_layout(
        height=1000,
        title_text=f"Seasonal Variation of {data_type.upper()}",
        showlegend=True,
        updatemenus=[dict(
            active=0,
            buttons=dropdown_buttons,
            direction="down",
            pad={"r": 10, "t": 10},
            showactive=True,
            x=0.1,
            xanchor="left",
            y=1.15,
            yanchor="top"
        )],
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1
        )
    )

    fig.update_xaxes(
        title_text="Month", 
        row=4, col=1, 
        tickmode='array', 
        tickvals=list(range(1, 13)),
        ticktext=month_names
    )
    for i in range(1, 5):
        fig.update_yaxes(title_text=f"SWC - Level {i}", row=i, col=1)

    # Make the first station visible by default
    for i in range(min(8, len(fig.data))):
        fig.data[i].visible = True

    return fig

def create_seasonal_plot_single_level(data, data_type):
    # Extract unique station prefixes
    stations = list(set([key.split('_')[0] for key in data.keys()]))
    
    fig = go.Figure()

    for station in stations:
        # Find the first key that matches this station prefix
        station_key = next(key for key in data.keys() if key.startswith(station))
        
        df = pd.DataFrame({
            'time': pd.to_datetime(data[station_key]['time']),
            'value': data[station_key]['data']
        })
        df['month'] = df['time'].dt.month
        df = df.dropna(subset=['value'])

        monthly_stats = df.groupby('month')['value'].agg(['mean', 'std']).reset_index()

        # Plot mean line
        fig.add_trace(go.Scatter(
            x=monthly_stats['month'],
            y=monthly_stats['mean'],
            mode='lines+markers',
            name=f'{station} - Mean',
            line=dict(color='blue', width=2),
            visible=False
        ))

        # Add error bars
        fig.add_trace(go.Scatter(
            x=monthly_stats['month'],
            y=monthly_stats['mean'],
            error_y=dict(
                type='data',
                array=monthly_stats['std'],
                visible=True
            ),
            mode='markers',
            marker=dict(color='red', size=8),
            name=f'{station} - Std Dev',
            visible=False
        ))

    # Create dropdown menu
    dropdown_buttons = []
    for i, station in enumerate(stations):
        visible = [False] * len(fig.data)
        visible[i*2] = True
        visible[i*2 + 1] = True
        
        dropdown_buttons.append(dict(
            label=station,
            method='update',
            args=[{'visible': visible},
                  {'title': f'Seasonal Variation of {data_type.upper()} for Station {station}'}]
        ))

    fig.update_layout(
        height=600,
        title_text=f"Seasonal Variation of {data_type.upper()}",
        showlegend=True,
        updatemenus=[dict(
            active=0,
            buttons=dropdown_buttons,
            direction="down",
            pad={"r": 10, "t": 10},
            showactive=True,
            x=0.1,
            xanchor="left",
            y=1.15,
            yanchor="top"
        )],
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1
        )
    )

    fig.update_xaxes(
        title_text="Month", 
        tickmode='array', 
        tickvals=list(range(1, 13)),
        ticktext=month_names
    )
    fig.update_yaxes(title_text=f"{data_type.upper()}")

    # Make the first station visible by default
    if len(fig.data) > 0:
        fig.data[0].visible = True
        fig.data[1].visible = True

    return fig
```


```{python}
create_seasonal_plot_multi_level(swc_filtered, 'swc')
```

```{python}
create_seasonal_plot_single_level(reco_swc_filtered, 'reco')

```

REDO THESE SEASNAL VARIATION PLOTS
DO THE SAME FOR RECO




## Outlook 

It would be great to find the locations of the statiosn that FLUXNET does not provide coordinate data for, such that there is more geographic data avilable to analyse. 

A next step for this project would be incorporating climatological data into the analysis - specifically precipitation and temperature - to better understand the relationship between SWC and RECO. Additionally, land cover classification for the stations could also be interesting to incorporate. 

