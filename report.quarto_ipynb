{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Soil Water Content and Ecosystem Respiration analysis\"\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "---\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This is an analysis of Soil Water Content (SWC) and and Ecosystem Respiration (RECO) data.\n",
        "\n",
        "GIVE A BIT OF BACKGROUND ON THE DATASET IE LEVELS ETC; JUSTIFY WHY ONLY 4 LEVELS\n",
        "\n",
        "First, lets import the modules we will need to complete the analysis:\n"
      ],
      "id": "8e40023d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "\n",
        "# data access and manipulation modules\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "# plotting libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go # interactive plots\n",
        "import matplotlib.dates as mdates\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "from matplotlib.backends.backend_pdf import PdfPages # saves multiple individual plots in 1 doc in case of not faceted plot\n",
        "from collections import defaultdict # for plotting layers of each station"
      ],
      "id": "8497e3c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, lets access the data we will be analysing.\n"
      ],
      "id": "b7e63612"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reco_data = './data/reco/'\n",
        "soil_water_data = './data/swc/'\n",
        "filtered_station_locations = './data/filtered-fluxnet-station-coords.csv'"
      ],
      "id": "ef3510bc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I wrote this code to also work with the many other variables and indices available from FLUXNET, so I wrote a lot of functions that can be used for many of them (rather than having to write a fresh code chunk each time). This is what now follows. Explanations of the function follows before each code block. \n",
        "\n",
        "The function below takes the datasets to be analysed, here SWC and RECO, and puts them into a more flexible form for te purposes of analysis and visualisation - a dictionary. The data directly from fluxnet is in the form of individual .txt files for each station. Here, I take these individual files and all their contents, and stitch them together into a dictionary, to be accessed by the station name as a key. \n"
      ],
      "id": "9e8f83de"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def process_data_files(directory):\n",
        "    data_dict = {}\n",
        "    for file in os.listdir(directory):\n",
        "        file_path = os.path.join(directory, file)\n",
        "        station_name = os.path.splitext(os.path.basename(file_path))[0].replace('_daily_avg', '')\n",
        "\n",
        "        time = []\n",
        "        data = []\n",
        "\n",
        "        with open(file_path, 'r') as datafile:\n",
        "            plotting = csv.reader(datafile, delimiter='\\t')\n",
        "            next(plotting)  # skips header row\n",
        "            for ROWS in plotting:\n",
        "                time.append(datetime.strptime(ROWS[0], '%Y-%m-%d'))\n",
        "                data.append(float(ROWS[1]))\n",
        "\n",
        "        data_dict[station_name] = {'time': time, 'data': data}\n",
        "\n",
        "    return data_dict\n",
        "  ```\n",
        "\n",
        "\n",
        "For some variables, it is interesting to take the log of its value; this is what this function does, and it creates a new directionary with the same name + log i.e. log_reco_data_dict.\n",
        "\n",
        " ```{python}\n",
        "  def log_transfo_data_dict(original_data_dict):\n",
        "    log_transformed_dict = {}\n",
        "\n",
        "    for station_name, data_info in original_data_dict.items():\n",
        "        log_transformed_data = np.log(data_info['data'])\n",
        "        log_transformed_dict[station_name] = {'time': data_info['time'], 'data': log_transformed_data}\n",
        "\n",
        "    return log_transformed_dict\n",
        "  ```\n",
        "\n",
        "\n",
        "  The function below takes whichever variables you want - again, here only SWC and RECO - and calculates various regression metrics of them, namely the r sqaured metric, the Pearson correlation coefficient, as well as the linear regression metrics (slope and intercept). \n",
        "\n",
        "\n",
        " ```{python}\n",
        "def calculate_regression_metrics(x, y):\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # find indices where either x or y is NaN\n",
        "    nan_indices = np.isnan(x) | np.isnan(y)\n",
        "\n",
        "    # exclude NaN values from both x and y\n",
        "    x_valid = x[~nan_indices]\n",
        "    y_valid = y[~nan_indices]\n",
        "\n",
        "    if len(x_valid) == 0 or len(y_valid) == 0:\n",
        "        # if no valid data points are available, return NaN for all metrics\n",
        "        return np.nan, np.nan, np.nan, np.nan\n",
        "\n",
        "    correlation_coefficient = np.corrcoef(x_valid, y_valid)[0, 1]\n",
        "\n",
        "    # linear regression for slope and intercept\n",
        "    slope, intercept = np.polyfit(x_valid, y_valid, 1)\n",
        "\n",
        "    residuals = y_valid - (slope * x_valid + intercept)\n",
        "    ss_residual = np.sum(residuals**2)\n",
        "    ss_total = np.sum((y_valid - np.mean(y_valid))**2)\n",
        "    r_squared = 1 - (ss_residual / ss_total)\n",
        "\n",
        "    return correlation_coefficient, r_squared, slope, intercept"
      ],
      "id": "68b8955c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the metrics are calculated,  it is necessary to ave them stored somewhere - so here is a function to store them in a dictionary. It extracts the prefix of the station for the first data dictioanry, which indicates its name, and compares it with the prefix from the second data dictionary. If the names match and the data is not empty at that station, then it moves on to the next step - else it throws an error message to alert you of the emoty data. The next step is to ensure we are only calculating the metrics over common time steps,; the data is thus cropped to the later beginning and earlier end of the 2 datasets. The metrics are then stored with astring as the key and the value as the value, and the whole thing is stored in the dictionary that was intialised at the beginning of the function.\n"
      ],
      "id": "aed175e7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def calculate_and_store_all_levels(dict1, dict2): \n",
        "    all_levels_metrics = {} # initialise emoty dictionary\n",
        "\n",
        "    for key1, values1 in dict1.items():\n",
        "      # the next 3 lines are to get the station name prefix out of the first dictionary\n",
        "        split_parts = key1.split('_')\n",
        "        station_name = split_parts[0]\n",
        "        level_name = split_parts[2]\n",
        "        matching_keys = [key2 for key2 in dict2.keys() if key2.startswith(station_name)]\n",
        "\n",
        "        if matching_keys:\n",
        "            for key2, values2 in dict2.items():\n",
        "                if key2.startswith(station_name) and 'data' in values2:\n",
        "                    x = np.array(values1.get('data', []))\n",
        "                    y = np.array(values2.get('data', []))\n",
        "\n",
        "                    if not (np.iterable(x) and np.iterable(y)):\n",
        "                        print(f\"Invalid data format for {key1} or {key2}\")\n",
        "                        continue\n",
        "\n",
        "                    if len(x) == 0 or len(y) == 0:\n",
        "                        print(f\"Empty data array for {key1} or {key2}\")\n",
        "                        continue\n",
        "\n",
        "                    min_len = min(len(x), len(y))\n",
        "                    x = x[:min_len]\n",
        "                    y = y[:min_len]\n",
        "\n",
        "                    correlation_coefficient, r_squared, slope, intercept = calculate_regression_metrics(x, y)\n",
        "\n",
        "                    level_metrics = {\n",
        "                        'correlation_coefficient': correlation_coefficient,\n",
        "                        'r_squared': r_squared,\n",
        "                        'slope': slope,\n",
        "                        'intercept': intercept\n",
        "                    }\n",
        "\n",
        "                    station_and_level = f\"{station_name}_{level_name}\"\n",
        "\n",
        "                    if station_and_level not in all_levels_metrics:\n",
        "                        all_levels_metrics[station_and_level] = {}\n",
        "\n",
        "                    all_levels_metrics[station_and_level][station_name] = level_metrics\n",
        "\n",
        "    return all_levels_metrics"
      ],
      "id": "789d1fad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I want to keep all my functions in one place, so I wont yet laod the results of the metric calculation; it will be found below, after the function definitions. \n",
        "\n",
        "If you want to save the results of all this work so far externally to a csv file, this is te function for you ! However, we wont do that here, since this is meant to be a standalone reproducible document. Here is the code chunk anyway: \n"
      ],
      "id": "4302f3ae"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "echo: true\n",
        "eval: false\n",
        "def metrics_to_csv(metrics_dict, name1, name2):\n",
        "\n",
        "    path = '/path/to/where/you/want/to/save/the/csv/'\n",
        "    file_name = f\"{name1}_{name2}_something_else.csv\"\n",
        "    file_path = path+file_name\n",
        "    with open(file_path, 'w', newline='') as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "\n",
        "        header = ['Station', 'Correlation Coefficient', 'R-squared', 'Slope', 'Intercept']\n",
        "        writer.writerow(header)\n",
        "\n",
        "\n",
        "        for station in metrics_dict.keys():\n",
        "            inner_dict = metrics_dict[station][next(iter(metrics_dict[station]))]\n",
        "            row = [station, inner_dict['correlation_coefficient'], inner_dict['r_squared'], inner_dict['slope'], inner_dict['intercept']]\n",
        "            writer.writerow(row)\n",
        "\n",
        "    print(f'Successfully saved to {file_path}')"
      ],
      "id": "ba6966f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As briefly mentioned above, the levels feature of the FLUXNET data is very interesting. So, we ave some functions to filter for that. First we need to check that the levels each have the same time step - it could be, that at a particular time step, levels 1,2, and 4 were recorded and not level 3, due to equipment failure for example. In this case, we do not want it, because we want a consistent time series. So we check first for this time step consistency:\n"
      ],
      "id": "43497535"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def remove_missing_time_steps(dict1):\n",
        "    for station_name, levels_data in gst_filtered.items():\n",
        "        station_prefix = station_name.split('_')[0]\n",
        "\n",
        "        level_keys = [key for key in dict1 if key.startswith(station_prefix)]\n",
        "\n",
        "        if len(level_keys) < 2:\n",
        "            continue\n",
        "\n",
        "        # create a dictionary to store time data for each level\n",
        "        time_data = {level: dict1[level]['time'] for level in level_keys}\n",
        "\n",
        "        reference_level = level_keys[0]  # first level as the reference\n",
        "        for current_level in level_keys[1:]:\n",
        "            # Update time for the current level\n",
        "            i = 0  # initialize index\n",
        "            while i < len(time_data[current_level]):\n",
        "                timestep = time_data[current_level][i]\n",
        "\n",
        "                if timestep not in time_data[reference_level]:\n",
        "                    print(f'Removing timestep {timestep} from {current_level}')\n",
        "\n",
        "                    # Remove corresponding 'data' value\n",
        "                    del dict1[current_level]['data'][i]\n",
        "\n",
        "                    # Remove the timestep from the list\n",
        "                    del time_data[current_level][i]\n",
        "                else:\n",
        "                    i += 1  # move to the next timestep\n",
        "\n",
        "            # Update dictionary with the modified time data\n",
        "            dict1[current_level]['time'] = time_data[current_level]\n",
        "\n",
        "    return dict1"
      ],
      "id": "86aa0371",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Something I want to look at is some patterns between levels and stations, which involes retaining time steps at each station that have data for 4 levels only - not 3, or less. That is what this function is for - to refine the dictionaries to only include the data if it exists in fulll i.e. at all levels. \n"
      ],
      "id": "249b296b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def four_levels_filter(data_dict, data_name):\n",
        "    desired_levels_stations = {}\n",
        "\n",
        "    for station_name in set(station.split('_')[0] for station in data_dict.keys()):\n",
        "        # initialize a dictionary for the station\n",
        "        station_data = {}\n",
        "\n",
        "        for station in data_dict:\n",
        "            if station.startswith(station_name):\n",
        "                # extract level from the station name\n",
        "                level = station.split('_')[-1][-2:]\n",
        "\n",
        "                # get data and time_steps from the dictionary\n",
        "                data = data_dict[station].get('data', [])\n",
        "                time_steps = data_dict[station].get('time', [])\n",
        "\n",
        "                # store data and time_steps in the station_data dictionary\n",
        "                station_data[level] = {'data': data, 'time': time_steps}\n",
        "\n",
        "        # check if the station has exactly 4 levels before storing in the dictionary\n",
        "        if len(station_data) == 4:\n",
        "            # Find common time steps among all levels\n",
        "            common_time_steps = set.intersection(*[set(station_data[level]['time']) for level in station_data])\n",
        "\n",
        "            # Filter data and time_steps to keep only common time steps\n",
        "            for level in station_data:\n",
        "                # Filter data and time_steps to keep only common time steps\n",
        "                station_data[level]['data'] = [value for value, ts in zip(station_data[level]['data'], station_data[level]['time']) if ts in common_time_steps]\n",
        "                station_data[level]['time'] = list(common_time_steps)\n",
        "\n",
        "            desired_levels_stations[station_name] = station_data\n",
        "\n",
        "            \n",
        " # Create a dictionary with the desired structure\n",
        "    four_levels_only_gst_dict = {}\n",
        "    \n",
        "    for station_name, levels_data in desired_levels_stations.items():\n",
        "        for level, data in levels_data.items():\n",
        "            new_key = f\"{station_name}_{data_name}_{level}\"\n",
        "            four_levels_only_gst_dict[new_key] = {'time': data['time'], 'data': data['data']}\n",
        "    \n",
        "    return four_levels_only_gst_dict"
      ],
      "id": "90c4c571",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function simply ensures that whichever 2 variables you want to play with cover the same period of time - if one i slonger than the other, the longer bit is removed by this function."
      ],
      "id": "303c3398"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def filter_level_data_common_time(dict1, dict2):    # order is important! dct1 is level data ie swc and dict2 is non leveled data ie reco\n",
        "    dict1_filtered = {}\n",
        "    dict2_filtered = {}\n",
        "\n",
        "    for station_name, data1 in dict2.items():\n",
        "        station_prefix = station_name.split('_')[0]\n",
        "      #  print(station_name)\n",
        "        for station_level_name, data2 in dict1.items():\n",
        "            if station_level_name.startswith(station_prefix):\n",
        "                level_name = station_level_name.split('_')[2] # the _l1_ etc bit of the name\n",
        "                # print(level_name)\n",
        "                #common_time_range = set(data1.get('time', [])).intersection(set(data2.get('time', [])))\n",
        "                common_time_range = [time for time in data1.get('time', []) if time in data2.get('time', [])]\n",
        "\n",
        "               # print(common_time_range)\n",
        "            \n",
        "                if common_time_range:\n",
        "\n",
        "                    dict1_key = f\"{station_level_name}\"\n",
        "                    dict1_filtered[dict1_key] = {'time': list(common_time_range), 'data': []}\n",
        "\n",
        "                    dict1_filtered[dict1_key]['data'] = [value for time, value in zip(data2['time'], data2['data']) if time in common_time_range]\n",
        "                    #print(len(dict1_filtered[dict1_key]['data']))\n",
        "\n",
        "                    dict2_key = f\"{station_name}_{level_name}\"  # make new name for reco with level in it\n",
        "                    dict2_filtered[dict2_key] = {'time': list(common_time_range), 'data': []}\n",
        "\n",
        "                    dict2_filtered[dict2_key]['data'] = [value for time, value in zip(data1['time'], data1['data']) if time in common_time_range]\n",
        "#                    print(len(dict2_filtered[dict2_key]['data']))\n",
        "\n",
        "    return dict1_filtered, dict2_filtered"
      ],
      "id": "5943888c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def box_plot_metrics_over_levels(metric_dict, metric_name, xlabel, ylabel):\n",
        "    # convert metric_dict to a pandas DataFrame for easier plotting\n",
        "    data = {'Level': [], 'Metric': [], 'Value': []}\n",
        "\n",
        "    for station_metric, station_metrics in metric_dict.items():\n",
        "        level_name = station_metric.split('_')[1]\n",
        "        correlation_coefficient = station_metrics[station_metric.split('_')[0]][metric_name] # bad naming here, but its for the metric inthe metrics to plot thing!\n",
        "\n",
        "        data['Level'].append(level_name)\n",
        "        data['Metric'].append(metric_name)\n",
        "        data['Value'].append(correlation_coefficient)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    levels = sorted(df['Level'].unique())  # Ensure levels are sorted\n",
        "    box_data = [df[df['Level'] == level]['Value'].tolist() for level in levels]\n",
        "    bp = ax.boxplot(box_data, labels=levels, showfliers=True, patch_artist=True)\n",
        "\n",
        "    # fill boxes with the same colour as levels from scatter\n",
        "    colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
        "    for box, color in zip(bp['boxes'], colors):\n",
        "        box.set(facecolor=color, alpha=0.7)\n",
        "    \n",
        "    ax.set_title(f'Box Plot of {metric_name}, stations with 4 levels only, common time steps between levels - {xlabel} and {ylabel}')\n",
        "    ax.set_xlabel('Level')\n",
        "    ax.set_ylabel(metric_name)\n",
        "    \n",
        "    return fig"
      ],
      "id": "f9be0094",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, that was a lot, but those functions make up most of the code! Now we can move on to some analysis. Lets use the 'process_data_files' function to stitch together our datasets, and also make a lof version of the RECO data set too, since that could be interesting:\n"
      ],
      "id": "f0f4265e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reco_data_dict = process_data_files(reco_data)\n",
        "log_reco_data_dict = log_transfo_data_dict(reco_data_dict)\n",
        "swc_data_dict = process_data_files(soil_water_data)"
      ],
      "id": "b02cf476",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we remove any inconsistencies in the levels data:\n"
      ],
      "id": "9a121a5e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# for retaining time steps with 4 levels only. so obvi do only with data that has levels (i.e. not reco!) \n",
        "\n",
        "remove_missing_time_steps(swc_data_dict) # if you want, you can name this and make it a seperate dictionary, to have one with 4 levels only, and one without the levels filtering. for te future of tis project, i am interested in the 4 levels only, so i will stick with that. "
      ],
      "id": "90f5432a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we filter for the stations have have exclusively 4 levels over the whole time series.\n"
      ],
      "id": "4ab08eab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "four_levels_swc = four_levels_filter(swc_data_dict, 'swc')"
      ],
      "id": "bd6c3803",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we simply crop the 2 data sets we want to compare - here SWC and RECO, and SWC and logRECO, such that they have the same period.\n"
      ],
      "id": "cca26136"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "swc_filtered, reco_swc_filtered = filter_level_data_common_time(four_levels_swc, reco_data_dict)\n",
        "swc_logfiltered, logreco_swc_filtered = filter_level_data_common_time(four_levels_swc, log_reco_data_dict)"
      ],
      "id": "38ae59dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we calculate the regression metrics for this data.\n"
      ],
      "id": "f07049f5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "swc_reco_four_levels_metrics = calculate_and_store_all_levels(swc_filtered, reco_swc_filtered)\n",
        "swc_four_levels_logreco_metrics = calculate_and_store_all_levels(swc_logfiltered, logreco_swc_filtered)"
      ],
      "id": "517e2d5a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ADD BOX PLOT PLOTS IN HERE\n",
        "\n",
        "If you wanted to save these boxplots as a pdf, use this function: \n"
      ],
      "id": "69e4cc27"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# just copy and paste! simply remember to change the name in the file path, and the variables specificied in the figure:\n",
        "\n",
        "echo: true\n",
        "eval: false\n",
        "\n",
        "metrics_to_plot = ['correlation_coefficient', 'r_squared', 'slope', 'intercept'] # to automate function call\n",
        "\n",
        "with PdfPages('C:/Users/violi/Documents/ESDSRS/STD/soil-moisture-ghc/results-test/4_levels_same_timesteps_swc_reco_box_plots.pdf') as pdf_pages:\n",
        "    for metric in metrics_to_plot:\n",
        "        fig = box_plot_metrics_over_levels(swc_reco_four_levels_metrics, metric, 'SWC', 'RECO')\n",
        "        pdf_pages.savefig(fig, bbox_inches='tight')\n",
        "        plt.close(fig)"
      ],
      "id": "515dad49",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And if you wanted to save these metrics as a csv file, use this code chunk:\n"
      ],
      "id": "a85c5305"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "echo: true\n",
        "eval: false\n",
        "\n",
        "metrics_to_csv(swc_reco_four_levels_metrics, 'SWC', 'RECO')\n",
        "metrics_to_csv(swc_four_levels_logreco_metrics, 'SWC', 'logRECO')"
      ],
      "id": "abbd46d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So far, so cool! But its lacking a bit of explicit spatio temporal energy. So, lets firstly plot the stations on a world map so we can see where we are looking at. \n",
        "\n",
        "\n",
        "This is how I filtered the full station coordinate data set for only the ones that were present in my final datasets. You dont need to run this in the final reproducible workflow - but include but Ive included it as a snippet for full reproducibility.\n"
      ],
      "id": "022503ff"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "echo: true\n",
        "eval: False\n",
        "df_coords = pd.read_csv(station_location_csv)\n",
        "station_names_from_dict = {key.split('_')[0] for key in swc_data_dict.keys()}\n",
        "df_filtered = df_coords[df_coords['station'].isin(station_names_from_dict)]\n",
        "\n",
        "# save the filtered DataFrame to a new CSV file\n",
        "output_csv_file_path = '/pat/to/save/filtered/coord/data/\n",
        "df_filtered.to_csv(output_csv_file_path, index=False)\n",
        "\n",
        "print(f\"Filtered CSV file has been saved to {output_csv_file_path}\")"
      ],
      "id": "869231cb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using this freshly filtered dataset, lets plot the locations on a really nice, interactive plot. You can zoom in and out here, and hover over a point to see the name!\n"
      ],
      "id": "7cefeaa3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# this is an interactive plot!! you can scroll and click!! and it tells you the name and coords of the station!!\n",
        "\n",
        "# load the location file\n",
        "df = pd.read_csv(filtered_station_locations)\n",
        "\n",
        "# ensure latitude and longitude are numeric\n",
        "df['lat'] = pd.to_numeric(df['lat'], errors='coerce')\n",
        "df['lon'] = pd.to_numeric(df['lon'], errors='coerce')\n",
        "\n",
        "# create the map\n",
        "fig = go.Figure(data=go.Scattergeo(\n",
        "    lon = df['lon'],\n",
        "    lat = df['lat'],\n",
        "    text = df['station'],  \n",
        "    mode = 'markers',\n",
        "    marker = dict(\n",
        "        size = 5,\n",
        "        color = 'red',\n",
        "        line = dict(\n",
        "            width = 1,\n",
        "            color = 'rgba(68, 68, 68, 0)'\n",
        "        )\n",
        "    )\n",
        "))\n",
        "\n",
        "# layout for a world map with just outlines\n",
        "fig.update_layout(\n",
        "    title = dict(\n",
        "        text = 'FLUXNET Station Locations',\n",
        "        x = 0.5,  # this centres the title\n",
        "        y = 0.95  # and this moves it down a little so its not touching the top\n",
        "    ),\n",
        "    geo = dict(\n",
        "        showland = True,\n",
        "        showcoastlines = True,\n",
        "        showcountries = True,\n",
        "        countrycolor = 'rgb(204, 204, 204)',\n",
        "        coastlinecolor = 'rgb(204, 204, 204)',\n",
        "        landcolor = 'rgb(255, 255, 255)',\n",
        "        projection_type = 'natural earth',  # changed to natural earth projection - can also do orthographic, mercator, equirectangular...etc etc\n",
        "        showocean = True,\n",
        "        oceancolor = 'rgb(255, 255, 255)',\n",
        "        showframe = False,\n",
        "        lonaxis = dict(\n",
        "            range = [-180, 180],  # crop longitude here if you dont want full map\n",
        "        ),\n",
        "        lataxis = dict(\n",
        "            range = [-90, 90],  # same with latitude\n",
        "        ),\n",
        "    ),\n",
        "    width = 1000,  # width of the figure\n",
        "    height = 600,  # height of the figure\n",
        "    margin = dict(l=0, r=0, t=30, b=0)  # reduce margins on sides\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "id": "05182150",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\violi\\anaconda3\\envs\\swc-reco\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}