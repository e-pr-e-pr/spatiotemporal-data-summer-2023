---
title: "Soil Water Content and Ecosystem Respiration Analysis"
author:  "**Eleanor Percy-Rouhaud**"  
subtitle: "Matriculation Number: 3749872"
format:
  html:
    code-fold: true
    theme: cosmo
jupyter: python3
execute:
  echo: true
  eval: true
  warning: false
---
```{python}
# need to load these first because i have aving some problems with loading them directly in this doc. the are exactly the same as here, just also stored in a seperate document! data_processing_functions.py
from data_processing_functions import process_data_files, log_transfo_data_dict, metrics_to_csv, filter_level_data_common_time, calculate_regression_metrics, calculate_and_store_all_levels, remove_missing_time_steps, four_levels_filter

import os
```


## Introduction 


This analysis focuses on Soil Water Content (SWC) and Ecosystem Respiration (RECO) data from FLUXNET. FLUXNET is a global network of micrometeorological measurement sites. FLUXNET uses eddy covariance techniques to measure the exchanges of carbon dioxide, water vapor, and energy between terrestrial ecosystems and the atmosphere. They provide high-frequency measurements of various environmental parameters, allowing for study of carbon, water, and energy fluxes.

In this study, I concentrate on two key variables:
- Soil Water Content (SWC): Measured at four depths (0.02m, 0.05m, 0.1m, 0.3m), SWC is vital for understanding water availability in the soil profile. It influences plant growth, soil microbial activity, and overall ecosystem functioning.
- Ecosystem Respiration (RECO): This represents the total carbon dioxide release from an ecosystem, including soil(microbial), plant, and animal respiration. While my primary interest is in soil respiration, RECO serves as a proxy due to data availability constraints.

What makes this dataset in particular interesting is that it contains levels in the SWC dataset. The SWC is measured at 4 different depths in the soil. While I didnt get too much time to look at this in this project, a goal is to look at the behaviour of SWC at 4 levels, and look for example for cross correlation in that data.  In this project, I did look at the regression metrics between RECO and SWC at different soil depths and across various stations to understand their spatial and vertical relationships. Additionally, I examine seasonal variations to capture temporal dynamics.

The relationship between these variables is complex and non-linear. Both SWC and temperature exhibit inflection points in their influence on soil respiration, where extremes (too high or too low) can decrease respiration and SWC rates. This study aims to explore these relationships and potentially uncover the roles of other factors like nitrogen or organic matter content.

The logarithmic transformation of RECO is employed to normalise the data distribution and potentially reveal relationships that might be obscured in the original scale.
This approach, utilising FLUXNET's extensive dataset, offers a unique opportunity to study ecosystem processes across diverse environments. By understanding the interplay between SWC and RECO, one gains insights into carbon fluxes and ecosystem functioning, which are crucial for predicting ecosystem responses to environmental changes.
Future extensions of this work could incorporate climatological variables like temperature and precipitation, as well as land cover analysis. 


This analysis aims to explore the relationship between SWC and RECO across various stations and levels. By examining these variables together, we can gain insights into how water availability influences carbon fluxes and overall ecosystem functioning. Ultimately though, this part of the project is an introductory, exploratory look at SWC and RECO, to gain some initial understanding of the relationship, and see how to continue the study. 



## Data sources and availability
All the data used in this analysis is from FLUXNET - here is the link straight to the data download page: https://fluxnet.org/data/download-data/. The data is free to download, but a user account must be made before the data can be downloaded. 

I downloaded the data via this website with point-and-click; I did not use a download script. The following images docment the step-by-step process of obtaining the data:


https://fluxnet.org/data/aboutdata/data-variables/ This page provides an overview of the data variables found in the data.

https://fluxnet.org/data/fluxnet2015-dataset/fullset-data-product/

Talk about fluxnet - eddy covariance site etc etc
CHECK DEPTHS BC I THINK I WROTE THEM WRONG

The README for the data is pretty much everything you need. 

Also mention the coordinate data!


## Setting up the functions to run the analysis

The functions listed here are in the order in which they are used in the analysis. 

First, lets import the modules we will need to complete the analysis:

```{python}

# data access and manipulation modules
import pandas as pd # mostly used to convert data into a workable format  i.e. a dataframe
import numpy as np # maths module
import math
import csv # to read csv files
import os

# plotting libraries
import matplotlib.pyplot as plt
import plotly.graph_objects as go # interactive plots
from plotly.subplots import make_subplots # for dropdown menu options in teractive plots!
import matplotlib.dates as mdates # for plotting dates nicely
import tabulate # to make markdown formatted table
import datetime
from datetime import datetime
from matplotlib.backends.backend_pdf import PdfPages # saves multiple individual plots in 1 doc in case of not faceted plot
from collections import defaultdict # for plotting layers of each station
```

Now, lets access the data we will be analysing.

```{python}
reco_data = './data/reco/'
soil_water_data = './data/swc/'
filtered_station_locations = './data/filtered-fluxnet-station-coords.csv'
```

I wrote this code to also work with the many other variables and indices available from FLUXNET, so I wrote a lot of functions that can be used for many of them (rather than having to write a fresh code chunk each time). This is what now follows. Explanations of the function follows before each code block for a quick overview; the functions contain docstrings with more detailed, technical explanations of the functions. You can use the code fold function to see the collapsed code. 

The function below takes the datasets to be analysed, here SWC and RECO, and puts them into a more flexible form for te purposes of analysis and visualisation - a dictionary. The data directly from fluxnet is in the form of individual .txt files for each station. Here, I take these individual files and all their contents, and stitch them together into a dictionary, to be accessed by the station name as a key. 

```{python}
def process_data_files(directory):
    '''
        Description:
            This function stitches together in individual stations' data and returns a dictionary with the station names as the keys, and the data (time, SWC/RECO) as the value. It is the first necessary step to getthe .txt format of the data into a more flexibile one to work with in the analysis. 

        Parameters:
            directory - A directory from which the files will be read in

        Return:
            data_dict - A dictionary of the file contents
    '''


    data_dict = {}
    for file in os.listdir(directory):
        file_path = os.path.join(directory, file)
        station_name = os.path.splitext(os.path.basename(file_path))[0].replace('_daily_avg', '')

        time = []
        data = []

        with open(file_path, 'r') as datafile:
            plotting = csv.reader(datafile, delimiter='\t')
            next(plotting)  # skips header row
            for ROWS in plotting:
                time.append(datetime.strptime(ROWS[0], '%Y-%m-%d'))
                data.append(float(ROWS[1]))

        data_dict[station_name] = {'time': time, 'data': data}

    return data_dict
```

For some variables, it is interesting to take the log of its value; this is what this function does, and it creates a new directionary with the same name + log i.e. log_reco_data_dict.


```{python}
def log_transfo_data_dict(original_data_dict):
    '''
    Description:
        This function takes the data dictionary created in the process_data_files function, and returns a dictionary, with the variable of interest being log transformed. It is used later for metric calculations, since it is sometimes interesting to look at the log transform of the variable..

    Parameters:
        data dictionary - a dictionary containing the data. Must be created by the proccess_data_files function, or have the same strucutre: data_dict = {station_name_level: {time: [datetime_list], data: [float_list]}}

    Return:
        data_dict - A dictionary of the file contents the same as the input parameter, just with the data variable having a log transformaiton applied
    '''
    log_transformed_dict = {}

    for station_name, data_info in original_data_dict.items():
        log_transformed_data = np.log(data_info['data'])
        log_transformed_dict[station_name] = {'time': data_info['time'], 'data': log_transformed_data}

    return log_transformed_dict
```

The function below is effectively tidying up the data a bit. It looks at the data that has levels - here SWC - and ensures they have a consistency in the time data. Since I want a consistent time series for this analysis, if some levels have data and others dont - I do not want them in my time series. This function removes those time steps, leaving a dataset with only 'complete' levels of data - i.e. 1, 1-2, 1, 2,3 and 1,2,3,4.

```{python}
#| eval: false
#| echo: true
#| output: false

def remove_missing_time_steps(dict1):
    '''
    Description:
        This function processes a dictionary of FLUXNET data to remove time steps that are missing in some levels but present in others. It ensures time consistency across different measurement levels for each station. The function operates on the data dictionary in-place, modifying it to maintain only time steps that are present in all levels of a given station.

    Parameters:
        dict1 - A dictionary containing FLUXNET data. The structure is expected to be:
                {station_name_level: {'time': [datetime_list], 'data': [float_list]}}
                where station_name_level is a string combining station name and measurement level.

    Return:
        dict1 - The same dictionary as input, but with missing time steps removed to ensure consistency across levels for each station. The structure remains the same, but some entries in 'time' and 'data' lists may have been removed.

    Key Operations:
        1. Groups data by station prefix.
        2. Uses the first level of each station as a reference for time steps.
        3. Compares other levels to the reference, removing time steps (and corresponding data) that don't exist in the reference level.
        4. Updates the dictionary in-place, ensuring all levels for a station have consistent time steps.

    '''
    for station_name, levels_data in dict1.items():
        station_prefix = station_name.split('_')[0]

        level_keys = [key for key in dict1 if key.startswith(station_prefix)]

        if len(level_keys) < 2:
            continue

        # create a dictionary to store time data for each level
        time_data = {level: dict1[level]['time'] for level in level_keys}

        reference_level = level_keys[0]  # first level as the reference
        for current_level in level_keys[1:]:
            # Update time for the current level
            i = 0  # initialise index
            while i < len(time_data[current_level]):
                timestep = time_data[current_level][i]

                if timestep not in time_data[reference_level]:

                    # Remove corresponding 'data' value
                    del dict1[current_level]['data'][i]

                    # Remove the timestep from the list
                    del time_data[current_level][i]
                else:
                    i += 1  # move to the next timestep

            # Update dictionary with the modified time data
            dict1[current_level]['time'] = time_data[current_level]

    return dict1
``` 

Once the inconsistent time steps have been removed, data is then filtered to be only the data with 4 full levels. I kept them as 2 seperate data sets for debugging purposes, and for future use it could be interesting to look only at parrticular levels, not only all 4. The following function filters the data for complete 4 levels, and stores it in a new dictionary:

```{python}
#| eval: false
#| echo: true


def four_levels_filter(data_dict, data_name):
    '''
    Description:
        This function processes a dictionary of FLUXNET data to filter and restructure it, keeping only stations with exactly four depth measurement levels. It ensures that all levels within a station have consistent time steps. The function creates a new dictionary with a standardised structure for further analysis.

    Parameters:
        data_dict - A dictionary containing FLUXNET data. The structure is expected to be:
                    {station_name_level: {'time': [datetime_list], 'data': [float_list]}}
        data_name - A string representing the name of the data variable being processed.

    Return:
        four_levels_only_gst_dict - A new dictionary containing only stations with four levels, 
                                    restructured for consistency. The new structure is:
                                    {station_name_data_name_level: {'time': [datetime_list], 'data': [float_list]}}. 'gst' in the return statement is an artifact from testing the function that I didn' remove. Please ignore the name there. 

    Key Operations:
        1. Groups data by station name.
        2. Filters to keep only stations with exactly four measurement levels.
        3. Finds common time steps across all levels for each station.
        4. Filters data to include only these common time steps, ensuring temporal consistency.
        5. Restructures the data into a new dictionary with a standardised format.

    '''
    desired_levels_stations = {}

    for station_name in set(station.split('_')[0] for station in data_dict.keys()):
        # initialise a dictionary for the station
        station_data = {}

        for station in data_dict:
            if station.startswith(station_name):
                # extract level from the station name
                level = station.split('_')[-1][-2:]

                # get data and time_steps from the dictionary
                data = data_dict[station].get('data', [])
                time_steps = data_dict[station].get('time', [])

                # store data and time_steps in the station_data dictionary
                station_data[level] = {'data': data, 'time': time_steps}

        # check if the station has exactly 4 levels before storing in the dictionary
        if len(station_data) == 4:
            # Find common time steps among all levels
            common_time_steps = set.intersection(*[set(station_data[level]['time']) for level in station_data])

            # Filter data and time_steps to keep only common time steps
            for level in station_data:
                # Filter data and time_steps to keep only common time steps
                station_data[level]['data'] = [value for value, ts in zip(station_data[level]['data'], station_data[level]['time']) if ts in common_time_steps]
                station_data[level]['time'] = list(common_time_steps)

            desired_levels_stations[station_name] = station_data

            
 # Create a dictionary with the desired structure
    four_levels_only_gst_dict = {}
    
    for station_name, levels_data in desired_levels_stations.items():
        for level, data in levels_data.items():
            new_key = f"{station_name}_{data_name}_{level}"
            four_levels_only_gst_dict[new_key] = {'time': data['time'], 'data': data['data']}
    
    return four_levels_only_gst_dict
``` 


This function simply ensures that whichever 2 variables you want to play with cover the same period of time - if one is longer than the other, the longer time series is cropped by this function. 
```{python}
#| eval: false
#| echo: true


def filter_level_data_common_time(dict1, dict2):    # order is important! dct1 is level data ie swc and dict2 is non leveled data ie reco
    '''
    Description:
        This function aligns two FLUXNET datasets: one resulting from the four_levels_filter function (dict1) 
        and another dataset (dict2) that may not have level-specific data. It ensures that both datasets 
        have matching time steps for each station and level, allowing for comparative analysis.

    Parameters:
        dict1 - A dictionary containing level-specific FLUXNET data, output from the four_levels_filter function.
                Structure: {station_name_data_name_level: {'time': [datetime_list], 'data': [float_list]}}
        dict2 - A dictionary containing non-level-specific FLUXNET data.
                Structure: {station_name: {'time': [datetime_list], 'data': [float_list]}}

    Return:
        dict1_filtered - technically no change with inpout dict1 - redefined here for naming consistency.
        dict2_filtered - A new dictionary derived from dict2, restructured to match dict1's format and 
                        filtered to include only common time steps with dict1.
                        New structure: {station_name_level: {'time': [datetime_list], 'data': [float_list]}}

    Key Operations:
        1. Iterates through stations in dict2 and matches them with corresponding levels in dict1.
        2. Identifies common time steps between matched datasets.
        3. Filters both datasets to include only these common time steps.
        4. Restructures dict2 to match the level-specific format of dict1.

    Note:
        This function is crucial for preparing FLUXNET data for comparative analysis between different 
        types of measurements, especially when one dataset has been pre-processed for four-level consistency. 
        It ensures temporal alignment between level-specific and level-free data, allowing for 
        accurate comparisons across different variables and measurement levels.
    '''
    dict1_filtered = {}
    dict2_filtered = {}

    for station_name, data1 in dict2.items():
        station_prefix = station_name.split('_')[0]
      #  print(station_name)
        for station_level_name, data2 in dict1.items():
            if station_level_name.startswith(station_prefix):
                level_name = station_level_name.split('_')[2] # the _l1_ etc bit of the name
                # print(level_name)
                #common_time_range = set(data1.get('time', [])).intersection(set(data2.get('time', [])))
                common_time_range = [time for time in data1.get('time', []) if time in data2.get('time', [])]

               # print(common_time_range)
            
                if common_time_range:

                    dict1_key = f"{station_level_name}"
                    dict1_filtered[dict1_key] = {'time': list(common_time_range), 'data': []}

                    dict1_filtered[dict1_key]['data'] = [value for time, value in zip(data2['time'], data2['data']) if time in common_time_range]
                    #print(len(dict1_filtered[dict1_key]['data']))

                    dict2_key = f"{station_name}_{level_name}"  # make new name for reco with level in it
                    dict2_filtered[dict2_key] = {'time': list(common_time_range), 'data': []}

                    dict2_filtered[dict2_key]['data'] = [value for time, value in zip(data1['time'], data1['data']) if time in common_time_range]
#                    print(len(dict2_filtered[dict2_key]['data']))

    return dict1_filtered, dict2_filtered
```


  The function below takes whichever variables you want - again, here only SWC and RECO - and calculates various regression metrics of them, namely the r sqaured metric, the Pearson correlation coefficient, as well as the linear regression metrics (slope and intercept). 

```{python}

def calculate_regression_metrics(x, y):
    '''
        Description:
            This function calculates the regression metrics between 2 given variables, x and y. It is used later in a function that stored these values in a meaningful way.

        Parameters:
            x - the data part of the data dictionaries - dict1[key1]['data']
            y - same as x, but a different data

        Return:
            metric1, metric2, metric3, metric 4 - 4 different metrics (correlation coefficient, r squared, slope and intercept)
    '''
    x = np.array(x)
    y = np.array(y)

    # find indices where either x or y is NaN
    nan_indices = np.isnan(x) | np.isnan(y)

    # exclude NaN values from both x and y
    x_valid = x[~nan_indices]
    y_valid = y[~nan_indices]

    if len(x_valid) == 0 or len(y_valid) == 0:
        # if no valid data points are available, return NaN for all metrics
        return np.nan, np.nan, np.nan, np.nan

    correlation_coefficient = np.corrcoef(x_valid, y_valid)[0, 1]

    # linear regression for slope and intercept
    slope, intercept = np.polyfit(x_valid, y_valid, 1)

    residuals = y_valid - (slope * x_valid + intercept)
    ss_residual = np.sum(residuals**2)
    ss_total = np.sum((y_valid - np.mean(y_valid))**2)
    r_squared = 1 - (ss_residual / ss_total)

    return correlation_coefficient, r_squared, slope, intercept
```

Once the metrics are calculated,  it is necessary to ave them stored somewhere - so here is a function to store them in a dictionary. It extracts the prefix of the station for the first data dictioanry, which indicates its name, and compares it with the prefix from the second data dictionary. If the names match and the data is not empty at that station, then it moves on to the next step - else it throws an error message to alert you of the emoty data. The next step is to ensure we are only calculating the metrics over common time steps,; the data is thus cropped to the later beginning and earlier end of the 2 datasets. The metrics are then stored with astring as the key and the value as the value, and the whole thing is stored in the dictionary that was intialised at the beginning of the function.

```{python}

def calculate_and_store_all_levels(dict1, dict2): 
    '''
        Description:
            This function calculates the regression metrics between 2 given variables, x and y. It is used later in a function that stored these values in a meaningful way.

        Parameters:
            dict1 - the data dictionaries as defined by 
            dict 2 -

        Return:
            metric1, metric2, metric3, metric 4 - 4 different metrics (correlation coefficient, r squared, slope and intercept)
    '''
    all_levels_metrics = {} # initialise emoty dictionary

    for key1, values1 in dict1.items():
      # the next 3 lines are to get the station name prefix out of the first dictionary
        split_parts = key1.split('_')
        station_name = split_parts[0]
        level_name = split_parts[2]
        matching_keys = [key2 for key2 in dict2.keys() if key2.startswith(station_name)]

        if matching_keys:
            for key2, values2 in dict2.items():
                if key2.startswith(station_name) and 'data' in values2:
                    x = np.array(values1.get('data', []))
                    y = np.array(values2.get('data', []))

                    if not (np.iterable(x) and np.iterable(y)):
                        print(f"Invalid data format for {key1} or {key2}")
                        continue

                    if len(x) == 0 or len(y) == 0:
                        print(f"Empty data array for {key1} or {key2}")
                        continue

                    min_len = min(len(x), len(y))
                    x = x[:min_len]
                    y = y[:min_len]

                    correlation_coefficient, r_squared, slope, intercept = calculate_regression_metrics(x, y)

                    level_metrics = {
                        'correlation_coefficient': correlation_coefficient,
                        'r_squared': r_squared,
                        'slope': slope,
                        'intercept': intercept
                    }

                    station_and_level = f"{station_name}_{level_name}"

                    if station_and_level not in all_levels_metrics:
                        all_levels_metrics[station_and_level] = {}

                    all_levels_metrics[station_and_level][station_name] = level_metrics

    return all_levels_metrics
```

I want to keep all my functions in one place, so I wont yet laod the results of the metric calculation; it will be found below, after the function definitions. 

If you want to save the results of all this work so far externally to a csv file, this is te function for you ! However, we wont do that here, since this is meant to be a standalone reproducible document. Here is the code chunk anyway: 

```{python}
def metrics_to_csv(metrics_dict, name1, name2):
    '''
    Description:
        This function takes a dictionary of calculated metrics and writes them to a CSV file. It's designed 
        to export statistical metrics (such as correlation coefficient, R-squared, slope, and intercept) 
        for different stations, likely derived from comparing two FLUXNET variables.

    Parameters:
        metrics_dict - A nested dictionary containing calculated metrics for each station.
                    Structure: {station: {metric_set: {metric_name: value}}}
        name1 - A string, likely representing the name of the first variable being compared.
        name2 - A string, likely representing the name of the second variable being compared.

    Return:
        None - The function doesn't return a value, but it creates a CSV file as a side effect.

    Key Operations:
        1. Constructs a file name and path based on the input parameters.
        2. Opens a new CSV file for writing.
        3. Writes a header row with column names.
        4. Iterates through the metrics dictionary, extracting values for each station.
        5. Writes a row for each station with its corresponding metric values.
        6. Prints a confirmation message with the file path upon successful completion.

    '''

    path = '/path/to/where/you/want/to/save/the/csv/'
    file_name = f"{name1}_{name2}_something_else.csv"
    file_path = path+file_name
    with open(file_path, 'w', newline='') as csv_file:
        writer = csv.writer(csv_file)

        header = ['Station', 'Correlation Coefficient', 'R-squared', 'Slope', 'Intercept']
        writer.writerow(header)


        for station in metrics_dict.keys():
            inner_dict = metrics_dict[station][next(iter(metrics_dict[station]))]
            row = [station, inner_dict['correlation_coefficient'], inner_dict['r_squared'], inner_dict['slope'], inner_dict['intercept']]
            writer.writerow(row)

    print(f'Successfully saved to {file_path}')
```
The following function plots the results of the above metric claculation visually, on an interactive world map. 

```{python}
def create_world_correlation_map(df, metrics, levels, data_type): # for now, ave to manually type in the variable for the title. want to imrove this to be dynamic later.
    '''
    Description:
        This function creates an interactive world map using Plotly to visualise correlation metrics between 2 variables  - i.e. soil water content (SWC) and ecosystem respiration (RECO) across different measurement levels. The map includes a dropdown menu to select different metrics and levels, updating the map dynamically based on the selection.

    Parameters:
        df - A Pandas DataFrame containing the data to be plotted. The DataFrame should include columns for longitude ('lon'), latitude ('lat'), station names ('station'), levels ('level'), and the metrics to be visualized.
        metrics - A list of strings representing the names of the metrics to be visualised (e.g., ['correlation_coefficient', 'r_squared']).
        levels - A list of strings or integers representing the measurement levels (e.g., ['L1', 'L2', 'L3', 'L4']).
        data_type - A string representing the type of data being compared with SWC (e.g., 'RECO').

    Return:
        fig - A Plotly Figure object containing the interactive world map with dropdown menu for selecting metrics and levels.

    Key Operations:
        1. Creates a base layer with all data points in light grey.
        2. Adds traces for each combination of metric and level, initially hidden.
        3. Configures a dropdown menu to toggle the visibility of traces based on the selected metric and level.
        4. Sets up the map layout, including geographic settings, size, margins, and annotations.
        5. Adds initial title and dropdown label annotations.

    '''
    fig = make_subplots(rows=1, cols=1, specs=[[{'type': 'scattergeo'}]])

    # Create a base layer with all data points
    fig.add_trace(go.Scattergeo(
        lon = df['lon'],
        lat = df['lat'],
        mode = 'markers',
        marker = dict(size = 8, color = 'lightgrey'),
        showlegend = False
    ))

    # Create traces for each combination of metric and level
    for metric in metrics:
        for level in levels:
            level_data = df[df['level'] == level]
            fig.add_trace(go.Scattergeo(
                lon = level_data['lon'],
                lat = level_data['lat'],
                text = level_data['station'] + '<br>' + metric + ': ' + level_data[metric].round(3).astype(str),
                mode = 'markers',
                marker = dict(
                    size = 8,
                    color = level_data[metric],
                    colorscale = 'Viridis',
                    showscale = True,
                  #  colorbar_title = metric
                ),
                name = f'{metric} - Level {level}',
                visible = False  # Start with all traces hidden
            ))

    # Make the first trace visible
    fig.data[1].visible = True

    # Create dropdown menu
    dropdown_buttons = []
    for i, metric in enumerate(metrics):
        for j, level in enumerate(levels):
            visible = [False] * len(fig.data)
            visible[0] = True  # Base layer always visible
            visible[1 + i*len(levels) + j] = True  # Make the selected trace visible
            dropdown_buttons.append(
                dict(
                    label = f'{metric.capitalize()} - Level {level}',
                    method = 'update',
                    args = [{'visible': visible},
                            {'annotations[1].text': f'{metric.capitalize()} between SWC and {data_type} for Level {level}, all stations'}]
                )
            )

    # Set the initial title
    initial_title = f'{metrics[0].capitalize()} between SWC and {data_type} for Level {levels[0]}, all stations'

    fig.update_layout(
        updatemenus=[dict(
            active=0,
            buttons=dropdown_buttons,
            direction="down",
            pad={"r": 10, "t": 10},
            showactive=True,
            x=1,
            xanchor="right",
            y=1.15,  # Position the dropdown menu
            yanchor="top"
        )],
        geo=dict(
            showland=True,
            showcountries=True,
            showocean=True,
            countrywidth=0.5,
            landcolor='rgb(243, 243, 243)',
            oceancolor='rgb(230, 230, 250)',
            projection_type='natural earth',
            showcoastlines=True,
            coastlinecolor="RebeccaPurple",
            showframe=False,
            lonaxis=dict(showgrid=False),
            lataxis=dict(showgrid=False)
        ),
        autosize=False,
        width=900,
        height=600,  # Increased height to accommodate title and dropdown
        margin=dict(l=0, r=0, t=150, b=0)  # Increased top margin for title and dropdown
    )
    
    # Add annotations for dropdown label and title
    fig.add_annotation(
        text="Select Metric and Level:",
        xref="paper", yref="paper",
        x=0.3, y=1.12,  # Position for dropdown label
        showarrow=False,
        font=dict(size=14)
    )
    
    fig.add_annotation(
        text=initial_title,
        xref="paper", yref="paper",
        x=0.5, y=1.06,  # Position for title
        showarrow=False,
        font=dict(size=16, color="black"),
        align="center"
    )
    
    return fig
```

The remaining functions are purely plotting functions. The following two are to plot the time series. There are two for the 2 cases - multilevel and single level data. It is cleaner to have these as 2 functions, rather than one function with many if loops to account for the presence or absence of levels. Like all the plots in tis report, it is interactive and collapsed to just one plot. This is in order to save space. Use the dropdown menu to select the plot of interest. 
```{python}
def create_timeseries_plot_single_level(data, data_type):
    '''
    Creates an interactive time series plot for single-level data.
    
    Parameters:
        data - A nested dictionary: {station_datatype_level: {'time': [datetime_list], 'data': [float_list]}}
        data_type - A string indicating the type of data being plotted
    
    Returns:
        fig - A Plotly Figure object
    '''
    # Extract unique stations
    stations = list(set([key.split('_')[0] for key in data.keys()]))
    
    # Create the figure
    fig = go.Figure()

    # Create traces for each station (initially hidden)
    for station in stations:
        # Find the first key that matches this station
        station_key = next(key for key in data.keys() if key.startswith(station))
        
        fig.add_trace(go.Scatter(
            x=data[station_key]['time'],
            y=data[station_key]['data'],
            mode='lines',
            name=station,
            visible=False
        ))

    # Create and add dropdown menu
    dropdown_buttons = []
    for i, station in enumerate(stations):
        visible = [False] * len(fig.data)
        visible[i] = True
        
        dropdown_buttons.append(dict(
            label=station,
            method='update',
            args=[{'visible': visible},
                  {'title': f'{data_type.upper()} Time Series for Station {station}'}]
        ))

    fig.update_layout(
        updatemenus=[dict(
            active=0,
            buttons=dropdown_buttons,
            direction="down",
            pad={"r": 10, "t": 10},
            showactive=True,
            x=0.1,
            xanchor="left",
            y=1.15,
            yanchor="top"
        )],
        height=600,
        title_text=f"Select a Station to view {data_type.upper()} Time Series",
        title_x=0.5,
        xaxis_title="Time",
        yaxis_title=data_type.upper()
    )

    # Make the first station visible by default
    if len(fig.data) > 0:
        fig.data[0].visible = True

    return fig
```

This is the same function as above, just with the functionality of plotting multiple levels on one plot, rather than just being a single time sereis. This is to account for the 4 levels of data present at each station.
```{python}
def create_timeseries_plot_multilevel(data):
    '''
    Creates an interactive time series plot for SWC data with multiple levels.
    
    Parameters:
        data - A nested dictionary: {station_swc_level: {'time': [datetime_list], 'data': [float_list]}}
    
    Returns:
        fig - A Plotly Figure object
    '''
    stations = list(set([key.split('_')[0] for key in data.keys()]))
    
    fig = go.Figure()

    for station in stations:
        for level in ['l1', 'l2', 'l3', 'l4']:
            key = f"{station}_swc_{level}"
            if key in data:
                fig.add_trace(go.Scatter(
                    x=data[key]['time'],
                    y=data[key]['data'],
                    mode='lines',
                    name=f'{level}',
                    visible=False
                ))

    dropdown_buttons = []
    for i, station in enumerate(stations):
        visible = [False] * len(fig.data)
        start_idx = i * 4
        for j in range(4):
            if start_idx + j < len(fig.data):
                visible[start_idx + j] = True
        
        dropdown_buttons.append(dict(
            label=station,
            method='update',
            args=[{'visible': visible},
                  {'title': f'SWC Time Series for Station {station}'}]
        ))

    fig.update_layout(
        updatemenus=[dict(
            active=0,
            buttons=dropdown_buttons,
            direction="down",
            pad={"r": 10, "t": 10},
            showactive=True,
            x=0.1,
            xanchor="left",
            y=1.15,
            yanchor="top"
        )],
        height=600,
        title_text="Select a Station to view SWC Time Series",
        title_x=0.5,
        xaxis_title="Time",
        yaxis_title="SWC"
    )

    # Make the first station visible by default
    for i in range(min(4, len(fig.data))):
        fig.data[i].visible = True

    return fig
```



The following functions creates an interactive plot of the seasonal variation of the data. The specific metrics plotted here are the mean, and the standard deviation. There are two functions, the only difference being that one plots the data with only one level, and the other one for ''multi level' data. It is easier to have 2 smaller functions than one big function with if loops determining the necessary plotting scheme. As you can see, the functions are still pretty big!


```{python}
#| eval: true
#| echo: false
month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']

def create_seasonal_plot_multi_level(data, data_type):

    '''
    Description:
        This function creates an interactive seasonal plot using Plotly to visualise FLUXNET data 
        across multiple levels for different stations. It generates 
        a subplot for each soil level, showing monthly averages and standard deviations.

    Parameters:
        data - A nested dictionary containing the time series data.
            Structure: {station_datatype_level: {'time': [datetime_list], 'data': [float_list]}}
        data_type - A string indicating the type of data being plotted (e.g., 'swc' for soil water content).

    Return:
        fig - A Plotly Figure object containing the interactive seasonal plot with subplots for each level 
            and a dropdown menu for selecting different stations.

    Key Operations:
        1. Creates a subplot for each of the four levels (L1, L2, L3, L4).
        2. For each station and level, calculates monthly statistics (mean and standard deviation).
        3. Plots mean values as lines with markers and standard deviations as error bars.
        4. Configures a dropdown menu to toggle visibility of traces for different stations.
        5. Sets up the plot layout, including titles, axis labels, and legend positioning.


    Note:
        The function assumes four levels of data for each station. It's designed to handle multiple 
        stations, allowing for comparative analysis of seasonal patterns across different locations along with the level information.
    '''
    stations = list(set([key.split('_')[0] for key in data.keys()]))
    
    fig = make_subplots(rows=4, cols=1, subplot_titles=("Level 1", "Level 2", "Level 3", "Level 4"),
                        shared_xaxes=True, vertical_spacing=0.05)
    levels = ['l1', 'l2', 'l3', 'l4']

    for station in stations:
        for i, level in enumerate(levels):
            key = f"{station}_{data_type}_{level}"
            if key in data:
                df = pd.DataFrame({
                    'time': pd.to_datetime(data[key]['time']),
                    'value': data[key]['data']
                })
                df['month'] = df['time'].dt.month
                df = df.dropna(subset=['value'])

                monthly_stats = df.groupby('month')['value'].agg(['mean', 'std']).reset_index()

                # Plot mean line
                fig.add_trace(go.Scatter(
                    x=monthly_stats['month'],
                    y=monthly_stats['mean'],
                    mode='lines+markers',
                    name=f'{station} - Mean',
                    line=dict(color='blue', width=2),
                    visible=False
                ), row=i+1, col=1)

                # Add error bars
                fig.add_trace(go.Scatter(
                    x=monthly_stats['month'],
                    y=monthly_stats['mean'],
                    error_y=dict(
                        type='data',
                        array=monthly_stats['std'],
                        visible=True
                    ),
                    mode='markers',
                    marker=dict(color='red', size=8),
                    name=f'{station} - Std Dev',
                    showlegend=False,
                    visible=False
                ), row=i+1, col=1)

    # Create dropdown menu
    dropdown_buttons = []
    for i, station in enumerate(stations):
        visible = [False] * len(fig.data)
        for j in range(8):  # 4 levels * 2 traces per level
            if i*8 + j < len(fig.data):
                visible[i*8 + j] = True
        
        dropdown_buttons.append(dict(
            label=station,
            method='update',
            args=[{'visible': visible},
                  {'title': f'Seasonal Variation of {data_type.upper()} for Station {station}'}]
        ))

    fig.update_layout(
        height=1000,
        title_text=f"Seasonal Variation of {data_type.upper()}",
        showlegend=True,
        updatemenus=[dict(
            active=0,
            buttons=dropdown_buttons,
            direction="down",
            pad={"r": 10, "t": 10},
            showactive=True,
            x=0.1,
            xanchor="left",
            y=1.15,
            yanchor="top"
        )],
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1
        )
    )

    fig.update_xaxes(
        title_text="Month", 
        row=4, col=1, 
        tickmode='array', 
        tickvals=list(range(1, 13)),
        ticktext=month_names
    )
    for i in range(1, 5):
        fig.update_yaxes(title_text=f"SWC - Level {i}", row=i, col=1)

    # Make the first station visible by default
    for i in range(min(8, len(fig.data))):
        fig.data[i].visible = True

    return fig

def create_seasonal_plot_single_level(data, data_type):
    '''
    Description:
        This function creates an interactive seasonal plot using Plotly to visualise FLUXNET data 
        (typically soil water content or another data type) for multiple stations at a single measurement level. 
        It generates a plot showing monthly averages and standard deviations for each station.

    Parameters:
        data - A nested dictionary containing the time series data.
            Structure: {station_datatype_level: {'time': [datetime_list], 'data': [float_list]}}
        data_type - A string indicating the type of data being plotted (e.g., 'swc' for soil water content).

    Return:
        fig - A Plotly Figure object containing the interactive seasonal plot with a dropdown menu 
            for selecting different stations.

    Key Operations:
    
        1. For each station and level, calculates monthly statistics (mean and standard deviation).
        2. Plots mean values as lines with markers and standard deviations as error bars.
        3. Configures a dropdown menu to toggle visibility of traces for different stations.
        4. Sets up the plot layout, including titles, axis labels, and legend positioning.


    Usage:
        This function is useful for visualising seasonal patterns of level-free data across multiple FLUXNET stations. 


    '''

    # Extract unique station prefixes
    stations = list(set([key.split('_')[0] for key in data.keys()]))
    
    fig = go.Figure()

    for station in stations:
        # Find the first key that matches this station prefix
        station_key = next(key for key in data.keys() if key.startswith(station))
        
        df = pd.DataFrame({
            'time': pd.to_datetime(data[station_key]['time']),
            'value': data[station_key]['data']
        })
        df['month'] = df['time'].dt.month
        df = df.dropna(subset=['value'])

        monthly_stats = df.groupby('month')['value'].agg(['mean', 'std']).reset_index()

        # Plot mean line
        fig.add_trace(go.Scatter(
            x=monthly_stats['month'],
            y=monthly_stats['mean'],
            mode='lines+markers',
            name=f'{station} - Mean',
            line=dict(color='blue', width=2),
            visible=False
        ))

        # Add error bars
        fig.add_trace(go.Scatter(
            x=monthly_stats['month'],
            y=monthly_stats['mean'],
            error_y=dict(
                type='data',
                array=monthly_stats['std'],
                visible=True
            ),
            mode='markers',
            marker=dict(color='red', size=8),
            name=f'{station} - Std Dev',
            visible=False
        ))

    # Create dropdown menu
    dropdown_buttons = []
    for i, station in enumerate(stations):
        visible = [False] * len(fig.data)
        visible[i*2] = True
        visible[i*2 + 1] = True
        
        dropdown_buttons.append(dict(
            label=station,
            method='update',
            args=[{'visible': visible},
                  {'title': f'Seasonal Variation of {data_type.upper()} for Station {station}'}]
        ))

    fig.update_layout(
        height=600,
        title_text=f"Seasonal Variation of {data_type.upper()}",
        showlegend=True,
        updatemenus=[dict(
            active=0,
            buttons=dropdown_buttons,
            direction="down",
            pad={"r": 10, "t": 10},
            showactive=True,
            x=0.1,
            xanchor="left",
            y=1.15,
            yanchor="top"
        )],
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1
        )
    )

    fig.update_xaxes(
        title_text="Month", 
        tickmode='array', 
        tickvals=list(range(1, 13)),
        ticktext=month_names
    )
    fig.update_yaxes(title_text=f"{data_type.upper()}")

    # Make the first station visible by default
    if len(fig.data) > 0:
        fig.data[0].visible = True
        fig.data[1].visible = True

    return fig
```

## Generating the results

So, that was a lot, but those functions make up most of the code! Now we can move on to some analysis. Lets use the 'process_data_files' function to stitch together our datasets, and also make a log version of the RECO data set too, since that could be interesting:

```{python}

# define  data paths
reco_data = './data/reco/'
soil_water_data = './data/swc/'
filtered_station_locations = './data/filtered-fluxnet-station-coords.csv'

# load the data into the dictionaries
reco_data_dict = process_data_files(reco_data)
log_reco_data_dict = log_transfo_data_dict(reco_data_dict)
swc_data_dict = process_data_files(soil_water_data)
```

First, we remove any inconsistencies in the levels data:

```{python}
#| output: false
# for retaining time steps with 4 levels only. so obvi do only with data that has levels (i.e. not reco!) 

remove_missing_time_steps(swc_data_dict) # if you want, you can name this and make it a seperate dictionary, to have one with 4 levels only, and one without the levels filtering. for te future of tis project, i am interested in the 4 levels only, so i will stick with that. 
```
Then we filter for the stations have have exclusively 4 levels over the whole time series.

```{python}
four_levels_swc = four_levels_filter(swc_data_dict, 'swc')
```

Then, we simply crop the 2 data sets we want to compare - here SWC and RECO, and SWC and logRECO, such that they have the same period.

```{python}
swc_filtered, reco_swc_filtered = filter_level_data_common_time(four_levels_swc, reco_data_dict)
swc_logfiltered, logreco_swc_filtered = filter_level_data_common_time(four_levels_swc, log_reco_data_dict)
```
Now, we calculate the regression metrics for this data.

```{python}
swc_reco_four_levels_metrics = calculate_and_store_all_levels(swc_filtered, reco_swc_filtered)
swc_four_levels_logreco_metrics = calculate_and_store_all_levels(swc_logfiltered, logreco_swc_filtered)
```


And if you wanted to save these metrics as a csv file, use this code chunk:

```{python}
#| eval: false
#| echo: true

metrics_to_csv(swc_reco_four_levels_metrics, 'SWC', 'RECO')
metrics_to_csv(swc_four_levels_logreco_metrics, 'SWC', 'logRECO')
```

So far, so cool! But its lacking a bit of explicit spatio temporal energy. So, lets firstly plot the stations on a world map so we can see where we are looking at. 


This is how I filtered the full station coordinate data set for only the ones that were present in my final datasets. You dont need to run this in the final reproducible workflow - but include but Ive included it as a snippet for full reproducibility.

```{python}

#| eval: false
#| echo: true
df_coords = pd.read_csv(station_location_csv)
station_names_from_dict = {key.split('_')[0] for key in swc_data_dict.keys()}
df_filtered = df_coords[df_coords['station'].isin(station_names_from_dict)]

# save the filtered DataFrame to a new CSV file
output_csv_file_path = '/pat/to/save/filtered/coord/data/
df_filtered.to_csv(output_csv_file_path, index=False)

print(f"Filtered CSV file has been saved to {output_csv_file_path}")
```
In this particular project, I am only interested in stations with 4 levels. Using the four levels filtered SWC dataset (for the identification of the sttaions with 4 levels in the coordinate dataset), and the filtered coordinate dataset, here is a map to illustrate where the FLUXNET eddy covariance stations are that we are looking at through the rest of this project:

```{python}

# Extract unique station names from swc_filtered keys
stations_in_swc = set(key.split('_')[0] for key in swc_filtered.keys())

# Load the location file
df = pd.read_csv(filtered_station_locations)

# Filter the dataframe to include only the stations in swc_filtered
df_filtered = df[df['station'].isin(stations_in_swc)]

# Ensure latitude and longitude are numeric
df_filtered['lat'] = pd.to_numeric(df_filtered['lat'], errors='coerce')
df_filtered['lon'] = pd.to_numeric(df_filtered['lon'], errors='coerce')

# Create the map
fig = go.Figure(data=go.Scattergeo(
    lon = df_filtered['lon'],
    lat = df_filtered['lat'],
    text = df_filtered['station'],  
    mode = 'markers',
    marker = dict(
        size = 8,  # Increased size for better visibility
        color = 'red',
        line = dict(
            width = 1,
            color = 'rgba(68, 68, 68, 0)'
        )
    )
))

# Layout for a world map with just outlines
fig.update_layout(
    title = dict(
        text = 'FLUXNET Station Locations (where only 4 levels are present)',
        x = 0.5,
        y = 0.95
    ),
    geo = dict(
        showland = True,
        showcoastlines = True,
        showcountries = True,
        countrycolor = 'rgb(204, 204, 204)',
        coastlinecolor = 'rgb(204, 204, 204)',
        landcolor = 'rgb(255, 255, 255)',
        projection_type = 'natural earth',
        showocean = True,
        oceancolor = 'rgb(255, 255, 255)',
        showframe = False,
        lonaxis = dict(
            range = [-180, 180],
        ),
        lataxis = dict(
            range = [-90, 90],
        ),
    ),
    width = 1000,
    height = 600,
    margin = dict(l=0, r=0, t=30, b=0)
)

fig.show()
```

This map shows the locations ADD TO THIS . This is done for visualisation purposes. 


I now visualise the results of the metric calculations. The visual representation on a map creates an instructive first look at the potential patterns in the data. Ive displayed it as a dropdown menu, to save some space on the page, and to easily find the variable one might be interested in. 

```{python}

# Load station locations
station_locations_df = pd.read_csv(filtered_station_locations)


# Transform the dictionary into a DataFrame
data = []
for key, value in swc_reco_four_levels_metrics.items():
    level = key.split('_')[1]
    for station, metrics in value.items():
        row = {'station': station, 'level': level}
        row.update(metrics)
        data.append(row)

swc_reco_metrics_df = pd.DataFrame(data)


# Merge location data with your metrics based on the station name
swc_reco_merged_df = swc_reco_metrics_df.merge(station_locations_df, on='station', how='left')

# Create the figure
metrics = ['correlation_coefficient', 'r_squared', 'slope', 'intercept']
levels = ['l1', 'l2', 'l3', 'l4']

fig = create_world_correlation_map(swc_reco_merged_df, metrics, levels, 'RECO')
fig.show()

```
And here is the same plot but with the logRECO and SWC regression calculations: 
```{python}

# Load station locations
station_locations_df = pd.read_csv(filtered_station_locations)


# Transform the dictionary into a DataFrame
data = []
for key, value in swc_four_levels_logreco_metrics.items():
    level = key.split('_')[1]
    for station, metrics in value.items():
        row = {'station': station, 'level': level}
        row.update(metrics)
        data.append(row)

swc_logreco_metrics_df = pd.DataFrame(data)


# Merge location data with your metrics based on the station name
swc_logreco_merged_df = swc_logreco_metrics_df.merge(station_locations_df, on='station', how='left')



# Create the figure
metrics = ['correlation_coefficient', 'r_squared', 'slope', 'intercept']
levels = ['l1', 'l2', 'l3', 'l4']

fig = create_world_correlation_map(swc_logreco_merged_df, metrics, levels, 'logRECO')
fig.show()
```

Here is a table of the results of the regression metrics between SWC and RECO, too:

```{python}
level_order = ['l1', 'l2', 'l3', 'l4']

swc_reco_metrics_df['level'] = pd.Categorical(swc_reco_metrics_df['level'], categories=level_order, ordered=True) # oterwise prints the table in a really topsy turrvy way
swc_reco_metrics_df = swc_reco_metrics_df.sort_values(['station', 'level'])

numeric_columns = ['correlation_coefficient', 'r_squared', 'slope', 'intercept']
swc_reco_metrics_df[numeric_columns] = swc_reco_metrics_df[numeric_columns].round(3)

markdown_table = swc_reco_metrics_df.to_markdown(index=False)

print(markdown_table)

```

And the same for SWC and logRECO:

```{python}
swc_logreco_metrics_df['level'] = pd.Categorical(swc_logreco_metrics_df['level'], categories=level_order, ordered=True) # oterwise prints the table in a really topsy turrvy way
swc_logreco_metrics_df = swc_logreco_metrics_df.sort_values(['station', 'level'])

numeric_columns = ['correlation_coefficient', 'r_squared', 'slope', 'intercept']
swc_logreco_metrics_df[numeric_columns] = swc_logreco_metrics_df[numeric_columns].round(3)

markdown_table = swc_logreco_metrics_df.to_markdown(index=False)

print(markdown_table)
```
Strong Positive Correlation: This could indicate that as soil moisture increases, ecosystem respiration increases. This might be observed in water-limited environments where increased water availability boosts microbial activity and plant respiration.
Weak or No Correlation: This might suggest that other factors (like temperature or nutrient availability) are more dominant in controlling ecosystem respiration at that site or time period.
Negative Correlation: This could occur in waterlogged conditions where excessive moisture limits oxygen availability, reducing microbial and root respiration.
The strength and nature of the correlation might change with seasons. For example, stronger correlations might be observed during growing seasons when plant activity is high.
Threshold Effects: You might observe threshold effects where the correlation changes dramatically above or below certain SWC levels, indicating optimal moisture ranges for ecosystem respiration.
Lag Effects: There might be time lags between changes in SWC and corresponding changes in RECO, which could affect the observed correlations.
Implications for Carbon Cycling: Strong correlations could indicate the importance of soil moisture in regulating ecosystem carbon fluxes, which has implications for understanding and modeling carbon cycle responses to climate change.

As can be seen from the output of this cell, some stations do not have location data. I am not really sure why this is; I checked again on the FLUXNET website, where the data is located, and they are not present there. Therefore the world map visualisations of the data do not feature the stations that do not have available location information. The following code confirms which stations are present in the location data table: 

```{python}
# checking which prefixes are present in the final dataset, just to see if I can Idenitfy tem. Taking into consideration the ones that dont have coordinates on FLUXNET, for some reason. 
unique_prefixes = set()

for key in swc_filtered.keys():
    # prefix extraction
    prefix = key.split('_')[0]
    unique_prefixes.add(prefix)

for prefix in unique_prefixes:
    print('the following stations have coordinates data ', prefix)
```


We can now look at te temporal evolution of SWC and RECO at the different stations. Here I am plotting only the stations I filtered for earlier with 4 levels. Since the dictionaries I use to structure the data have the same structure, you could also use the non filtered data (swc_data_dict, for eaxmple) to see more stations. 

Again, this is an interactive plot set up featuring a drop down menu, for space saving reasons. 



```{python}
#| eval: true
#| echo: false
create_timeseries_plot_multilevel(swc_filtered)
```
And here is the RECO time series: 

```{python}
#| eval: true
#| echo: false
create_timeseries_plot_single_level(reco_swc_filtered, 'RECO')
```


These plots are very messy, and a lot of them feature sections with very flat lines. These are simply points where, for a period of time, data at that level was not recorded for whatever reason, or that the time steps were removed with the 'remove_missing_time_steps' function, for consistency across the time series. The lines are purely visual (I couldnt get them out in time :( ) and are not actual data. Even though this raw data is quite messy, we can still see 2 interesting things - the behaviour of SWC at different depths, and, in many cases, a strong seasonality. So then decided to look at the seasonal trends.

Below I discuss some stations that show particularly interesting or unique patterns

the Soil water Content at station FI-Sii shows really strong differences between levels. This location is in Finland, and the time series runs over 3 years - not super long, but long enough to see some patterns. Level 1 data, as a reminder, is the shallowest measurement closest to the surface. Level 4 is the deepest measurement at ADD THE DEPTH VALUE IN HERE. In general, this station shows a really high SWC between 80 - 90 % for most of the year. The particularly interesting thing to note is the behaviour as it changes with depth - the 2 deeper measurements (levels 3 and 4) are pretty stable the whole year through. The shallower levels - 1 and 2 - show fluctuating behaviour based on the time of year. Especially in the case of the level 1 data, one can see that the summer months affected this dramatically - SWC is level 1 drops quickly starting around June, and reaches its minimum of 20 percent around August//September. Tihs strong seasonality is also seen in the RECO temporal plot. 

BE-Maa (Belgium) SWC time series plot shows alsoa very clear pattern, but upon closer inspection, in a really odd way. Level 2 data at this station has pretty much consistently the lowest SWC value. I dont have an idea yet as to why this could be. This could be very well further investigated with land cover analysis, and soil structure analysis. The general pattern between the levels is very similar. Seasonality is also apparent - one can see the winter peaks and summer lows. It could also be said that the summer of 2020, just before July, was probably much wetter than normal. Climatological data is necessary to confirm this statement.

Finally, FR-LBr is interesting to look at simply because it has the opposite structure of FI-Sii - level 1 consistently has the highest SWC, and level 4 the lowest. 

FR-Hes is an example of a long time series with very clear seasonality. 

Viewing these plots and noticing these differences and similarities is very instructive to design the next phase of the project. The fact there is not one singular pattern of the order of the levels (in terms of highest and lowest SWC) suggests much more needs to be done to understand and predict the patterns. Incorporating climataological variables such as precipitation and temperature, as well s land cover analysis and soil strucuture analysis, could offer more insight into this. 

Finally, I was interested to look at the monthly seasonal variation of these variables, since some of the time series plots were too messy to really grasp. The monthly mean of the data combined with the standard deviation. This helps identify recurring patterns.  Combined with other data (climatoloigcal, land cover analysis etc), this can reveal how ceertain factors consiistently influence the data during specific times of the year. Additionally, by understanding the seasonal variation, it becomes easier to predict future values more accurately, which is crucial for forecasting, and udnerstanding the changing climate. The standard deviation provides a measure of the variability of the data points around the mean. This helps in understanding the extent of fluctuations during different months and identifying periods of high or low variation.

```{python}
create_seasonal_plot_multi_level(swc_filtered, 'swc')
```

```{python}
create_seasonal_plot_single_level(reco_swc_filtered, 'reco')

```


As mentioned before (with the plots of locations of available stations), almost all the stations are in Europe. There is one in South America, CHEKC IF I ACTUALLY MENTIONED IT BEFORE

DISCUSS THESE PLOTS MORE

smth like we observe a general trend of the data in northern europe being ..... however, there is an outlier in ... which is interesting. this difference could be accounted for by considering .... More data and research is needed to confirm this. 

also about guyana station - SWC consistencies with depth

## Summary of results

## Outlook 

It would be great to find the locations of the stations that FLUXNET does not provide coordinate data for, such that there is more geographic data avilable to analyse. Of particular note is that there are  no stations present in the Southern hemisphere. 

A next step for this project would be incorporating climatological data into the analysis - specifically precipitation and temperature - to better understand the relationship between SWC and RECO. Additionally, land cover classification for the stations could also be interesting to incorporate. Finally, soil structure could also be interesting - in the example of the SWC temporal plots at station FI-Sii, the dramatic differences in SWC over a relatively small depth could be explained by knowledge of the soil structure - whether it holds water well, or not, for example. This could then give practical information when combined with land cover analysis, about what kinds of soils and plants retaining a more stable soil water content. This information could help people adapt to climate change and still be able to grow crops and 

### Note

The 'h' on my keyboard is not working properly - I tried to catch all the mistakes, but if there are some words missing an h - please be forgiving! :) 

